        在机器学习中，损失函数是代价函数的一部分，而代价函数则是目标函数的一种类型[[1]](https://link.zhihu.com/?target=https%3A//stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)。机器学习中的监督学习本质上是给定一系列训练样本![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28x_%7Bi%7D%2C+y_%7Bi%7D%5Cright%29)，尝试学习![[公式]](https://www.zhihu.com/equation?tex=x+%5Crightarrow+y)的映射关系，使得给定一个![[公式]](https://www.zhihu.com/equation?tex=x)，即便这个![[公式]](https://www.zhihu.com/equation?tex=x)不在训练样本中，也能够输出![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)，尽量与真实的![[公式]](https://www.zhihu.com/equation?tex=y)接近。损失函数是用来估量**模型的输出**![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)与**真实值**![[公式]](https://www.zhihu.com/equation?tex=y)之间的差距，给模型的优化指引方向。模型的结构风险包括了经验风险和结构风险，损失函数是经验风险函数的核心部分：

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D%3D%5Carg+%5Cmin+_%7B%5Ctheta%7D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+L%5Cleft%28y_%7Bi%7D%2C+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29%2B%5Clambda+%5CPhi%28%5Ctheta%29%5Cright%29+%5C%5C)

式中，前面的均值函数为**经验风险**，![[公式]](https://www.zhihu.com/equation?tex=L%5Cleft%28y_%7Bi%7D%2C+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29%5Cright%29)为损失函数，后面的项为**结构风险**，![[公式]](https://www.zhihu.com/equation?tex=%5CPhi%28%5Ctheta%29)衡量模型的复杂度。

**首先区分损失函数、代价函数和目标函数之间的区别和联系：**

- **损失函数（Loss Function）通常是针对单个训练样本而言**，给定一个模型输出![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)和一个真实值![[公式]](https://www.zhihu.com/equation?tex=y)，损失函数输出一个实值损失![[公式]](https://www.zhihu.com/equation?tex=L%3Df%5Cleft%28y_%7Bi%7D%2C+%5Chat%7By%7D_%7Bi%7D%5Cright%29)，比如说：

- 线性回归中的均方差损失：![[公式]](https://www.zhihu.com/equation?tex=L%28y_%7Bi%7D%2C+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29%3D%5Cleft%28f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29-y_%7Bi%7D%5Cright%29%5E%7B2%7D)

- SVM中的Hinge损失：![[公式]](https://www.zhihu.com/equation?tex=L%28y_%7Bi%7D%2C+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29%3D%5Cmax+%5Cleft%280%2C1-f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29+y_%7Bi%7D%5Cright%29)

- 精确度定义中的0/1损失：![[公式]](https://www.zhihu.com/equation?tex=L%28y_%7Bi%7D%2C+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29%3D1%29%5CLongleftrightarrow+f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29+%5Cneq+y_%7Bi%7D)

- **代价函数（Cost Function）通常是针对整个训练集**（或者在使用mini-batch gradient descent时的一个mini-batch）的总损失![[公式]](https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D+f%5Cleft%28y_%7Bi%7D%2C+%5Chat%7By%7D_%7Bi%7D%5Cright%29)，比如说：

- 均方误差：![[公式]](https://www.zhihu.com/equation?tex=M+S+E%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28f%5Cleft%28x_%7Bi%7D+%3B+%5Ctheta%5Cright%29-y_%7Bi%7D%5Cright%29%5E%7B2%7D)

- SVM的代价函数：![[公式]](https://www.zhihu.com/equation?tex=S+V+M%28%5Ctheta%29%3D%5C%7C%5Ctheta%5C%7C%5E%7B2%7D%2BC+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cxi_%7Bi%7D)

- **目标函数（Objective Function）通常是一个更通用的术语**，表示任意希望被优化的函数，用于机器学习领域和非机器学习领域（比如运筹优化），比如说，最大似然估计（MLE）中的似然函数就是目标函数。

一句话总结三者的关系就是：A loss function is a part of a cost function which is a type of an objective function。

        损失函数是用于衡量模型所作出的预测离真实值（**Ground Truth**）之间的偏离程度。 通常，我们都会最小化目标函数，最常用的算法便是“梯度下降法”（**Gradient Descent**）。俗话说，任何事情必然有它的两面性，因此，并没有一种万能的损失函数能够适用于所有的机器学习任务，所以在这里我们需要知道每一种损失函数的优点和局限性，才能更好的利用它们去解决实际的问题。损失函数大致可分为两种：回归损失（针对**连续型**变量）和分类损失（针对**离散型**变量）。

<img src="https://pic3.zhimg.com/v2-030170854ff3560445c852f30be6455e_b.jpg" title="" alt="" data-align="center">

## 1. 回归损失（Regression Loss）

## 1.1 L1 Loss

        也称为**Mean Absolute Error**，即平均绝对误差（**MAE**），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。平均绝对误差（Mean Absolute Error Loss，MAE）是常用的损失函数，也称为L1 Loss。其基本形式如下：

![[公式]](https://www.zhihu.com/equation?tex=J_%7BM+A+E%7D%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5C%5C)

**背后的假设：**

        同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式，假设**模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution**![[公式]](https://www.zhihu.com/equation?tex=%28%5Cmu%3D0%2C+b%3D1%29)，则给定一个![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D)模型输出真实值![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D)的概率为：

![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D+%7C+x_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B2%7D+%5Cexp+%5Cleft%28-%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C%5Cright%29+%5C%5C)

        与推导 MSE 时类似，我们可以得到的负对数似然（**Negative Log-Likelihood**）实际上就是MAE 损失的形式：

![[公式]](https://www.zhihu.com/equation?tex=L%28x%2C+y%29%3D%5Cprod_%7Bi%3D1%7D%5E%7BN%7D+%5Cfrac%7B1%7D%7B2%7D+%5Cexp+%5Cleft%28-%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C%5Cright%29+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=L+L%28x%2C+y%29%3D-%5Cfrac%7BN%7D%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=N+L+L%28x%2C+y%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5C%5C)

**优点：** 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。

**缺点：** 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。

## 1.2 L2 Loss

        也称为**Mean Squred Error**，即均方差（**MSE**），它衡量的是预测值与真实1值之间距离的平方和，作用范围同为0到正无穷。均方差（Mean Squared Error，MSE）损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：

![[公式]](https://www.zhihu.com/equation?tex=J_%7BM+S+E%7D%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D+%5C%5C)

**背后的假设：**

        实际上在一定的假设下，我们可以使用最大化似然得到均方差损失的形式。假设**模型预测与真实值之间的误差服从标准高斯分布**![[公式]](https://www.zhihu.com/equation?tex=%28%5Cmu%3D0%2C+%5Csigma%3D1%29)，则给定一个![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D)，模型就输出真实值![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D)的概率为：

![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D+%7C+x_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi%7D%7D+%5Cexp+%5Cleft%28-%5Cfrac%7B%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B2%7D%5Cright%29+%5C%5C)

        进一步我们假设数据集中N个样本点之间相互独立，则给定所有![[公式]](https://www.zhihu.com/equation?tex=x)输出所有真实值![[公式]](https://www.zhihu.com/equation?tex=y)的概率，即似然（**Likelihood**）为所有![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D+%7C+x_%7Bi%7D%5Cright%29)的累乘：

![[公式]](https://www.zhihu.com/equation?tex=L%28x%2C+y%29%3D%5Cprod_%7Bi%3D1%7D%5E%7BN%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi%7D%7D+%5Cexp+%5Cleft%28-%5Cfrac%7B%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B2%7D%5Cright%29+%5C%5C)

通常为了计算方便，我们通常最大化对数似然（**Log-Likelihood**）：

![[公式]](https://www.zhihu.com/equation?tex=L+L%28x%2C+y%29%3D%5Clog+%28L%28x%2C+y%29%29%3D-%5Cfrac%7BN%7D%7B2%7D+%5Clog+2+%5Cpi-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D+%5C%5C)

去掉与![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D)无关的第一项，然后转化为最小化负对数似然（**Negative Log-Likelihood**）：

![[公式]](https://www.zhihu.com/equation?tex=N+L+L%28x%2C+y%29%3D%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D+%5C%5C)

        可以看到这个实际上就是均方差损失的形式。也就是说**在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的**，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。

**优点：** 对离群点（**Outliers**）或者异常值更具有鲁棒性。

**缺点：** 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。

**MAE与MSE的区别：**

- MSE比MAE能够更快收敛：当使用梯度下降算法时，MSE损失的梯度为![[公式]](https://www.zhihu.com/equation?tex=-%5Chat%7By%7D_%7Bi%7D)，而MAE损失的梯度为![[公式]](https://www.zhihu.com/equation?tex=%5Cpm+1)。所以。MSE的梯度会随着误差大小发生变化，而MAE的梯度一直保持为1，这不利于模型的训练
- MAE对异常点更加鲁棒：从损失函数上看，MSE对误差平方化，使得异常点的误差过大；从两个损失函数的假设上看，MSE假设了误差服从高斯分布，MAE假设了误差服从拉普拉斯分布，拉普拉斯分布本身对于异常点更加鲁棒

![](https://pic2.zhimg.com/v2-594296ac168ae6b61dde20194adbae35_r.jpg)

> 对于L1范数和L2范数，如果异常值对于实际业务非常重要，我们可以使用MSE作为我们的损失函数；另一方面，如果异常值仅仅表示损坏的数据，那我们应该选择MAE作为损失函数。此外，考虑到收敛速度，在大多数的卷积神经网络中（CNN）中，我们通常会选择L2损失。但是，还存在这样一种情形，当你的业务数据中，存在95%的数据其真实值为1000，而剩下5%的数据其真实值为10时，如果你使用MAE去训练模型，则训练出来的模型会偏向于将所有输入数据预测成1000，因为MAE对离群点不敏感，趋向于取中值。而采用MSE去训练模型时，训练出来的模型会偏向于将大多数的输入数据预测成10，因为它对离群点异常敏感。因此，大多数情况这两种回归损失函数并不适用，能否有什么办法可以同时利用这两者的优点呢？

## 1.3 Smooth L1 Loss

        即平滑的L1损失（SLL），出自[Fast RCNN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1504.08083) [7]。SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|>1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。

<img src="https://pic2.zhimg.com/v2-dda471c3e8b9556e920569e5511d2859_b.jpg" title="" alt="" data-align="center">

        通过对这三个损失函数进行求导可以发现，L1损失的导数为常数，如果不及时调整学习率，那么当值过小时，会导致模型很难收敛到一个较高的精度，而是趋向于一个固定值附近波动。反过来，对于L2损失来说，由于在训练初期值较大时，其导数值也会相应较大，导致训练不稳定。最后，可以发现Smooth L1在训练初期输入数值较大时能够较为稳定在某一个数值，而在后期趋向于收敛时也能够加速梯度的回传，很好的解决了前面两者所存在的问题。

![](https://pic4.zhimg.com/v2-d2e90f129b1cf9f666eb39e760711f63_b.jpg)

## 1.4 IoU Loss

        即交并比损失，出自[UnitBox](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1608.01471) [8]，由旷视科技于ACM2016首次提出。常规的Lx损失中，都是基于目标边界中的4个坐标点信息之间分别进行回归损失计算的。因此，这些边框信息之间是相互**独立**的。然而，直观上来看，这些边框信息之间必然是存在某种**相关性**的。如下图(a)-(b)分别所示，绿色框代表Ground Truth，黑色框代表Prediction，可以看出，同一个Lx分数，预测框与真实框之间的拟合/重叠程度并不相同，显然**重叠度**越高的预测框是越合理的。IoU损失将候选框的四个边界信息作为一个**整体**进行回归，从而实现准确、高效的定位，具有很好的尺度不变性。为了解决IoU度量不可导的现象，引入了负Ln范数来间接计算IoU损失。

<img src="https://pic2.zhimg.com/v2-c098f35067e564f95ae197748b4528ed_b.jpg" title="" alt="" data-align="center">

![](https://pic2.zhimg.com/v2-25c0a1a8477b14dfea09044f94cce9c9_b.jpg)

## 1.5 GIoU Loss

        即泛化的IoU损失，全称为Generalized Intersection over Union，由斯坦福学者于CVPR2019年发表的这篇[论文](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1902.09630) [9]中首次提出。上面我们提到了IoU损失可以解决边界框坐标之间相互独立的问题，考虑这样一种情况，当预测框与真实框之间没有任何重叠时，两个边框的交集（分子）为0，此时IoU损失为0，因此IoU无法算出两者之间的距离（重叠度）。另一方面，由于IoU损失为零，意味着梯度无法有效地反向传播和更新，即出现梯度消失的现象，致使网络无法给出一个**优化的方向**。此外，如下图所示，IoU对**不同方向的边框对齐**也是一脸懵逼的，所计算出来的值都一样。

<img src="https://pic1.zhimg.com/v2-969e5092d2d9532e0b9f867889e590e8_b.jpg" title="" alt="" data-align="center">

        为了解决以上的问题，如下图公式所示，GIoU通过计算任意两个形状（这里以矩形框为例）A和B的一个**最小闭合凸面**C，然后再计算C中排除掉A和B后的面积占C原始面积的比值，最后再用原始的IoU减去这个比值得到泛化后的IoU值。

![](https://pic2.zhimg.com/v2-57a550766079f00236011bad06a2cab9_b.jpg)

        GIoU具有IoU所拥有的一切特性，如对称性，三角不等式等。GIoU ≤ IoU，特别地，0 ≤ IoU(A, B) ≤ -1, 而0 ≤ GIoU(A, B) ≤ -1；当两个边框的完全重叠时，此时GIoU = IoU = 1. 而当 |AUB| 与 最小闭合凸面C 的比值趋近为0时，即两个边框不相交的情况下，此时GIoU将渐渐收敛至 -1. 同样地，我们也可以通过一定的方式去计算出两个矩形框之间的GIoU损失，具体计算步骤也非常简单，详情参考原论文。

## 1.6 DIoU Loss

        即距离IoU损失，全称为Distance-IoU loss，由天津大学数学学院研究人员于AAAI2020所发表的这篇[论文](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.08287) [10]中首次提出。上面我们谈到GIoU通过引入最小闭合凸面来解决IoU无法对不重叠边框的优化问题。但是，其仍然存在两大局限性：**边框回归还不够精确 & 收敛速度缓慢** 。考虑下图这种情况，当目标框**完全包含**预测框时，此时GIoU**退化**为IoU。显然，我们希望的预测是最右边这种情况。因此，作者通过计算两个边框之间的**中心点归一化距离**，从而更好的优化这种情况。

<img src="https://pic2.zhimg.com/v2-65871344c469f6b4fbba499bbc759f5d_b.jpg" title="" alt="" data-align="center">

        下图表示的是GIoU损失（第一行）和DIoU损失（第二行）的一个训练过程收敛情况。其中绿色框为目标边框，黑色框为锚框，蓝色框和红色框则分别表示使用GIoU损失和DIoU损失所得到的预测框。可以发现，GIoU损失一般会增加预测框的大小使其能和目标框重叠，而DIoU损失则直接使目标框和预测框之间的中心点归一化距离最小，即让预测框的中心快速的向目标中心收敛。

<img src="https://pic4.zhimg.com/v2-7c8bd04e67bb314f47dbadb66be5a1f3_b.jpg" title="" alt="" data-align="center">

        左图给出这三个IoU损失所对应的计算公式。对于DIoU来说，如图右所示，其惩罚项由两部分构成：分子为目标框和预测框中心点之间的欧式距离；分母为两个框最小外接矩形框的两个对角线距离。因此， 直接优化两个点之间的距离会使得模型**收敛得更快，同时又能够在两个边框不重叠的情况下给出一个优化的方向。**

<img src="https://pic3.zhimg.com/v2-42182127096913f2c06640bd363c783e_b.jpg" title="" alt="" data-align="center">

## 1.7 CIoU Loss

        即完整IoU损失，全称为Complete IoU loss，与DIoU出自同一篇论文。上面我们提到GIoU存在两个缺陷，DIoU的提出解决了其实一个缺陷，即收敛速度的问题。而一个好的边框回归损失应该同时考虑三个重要的几何因素，即重叠面积（**Overlap area**）、中心点距离（C**entral point distance**）和高宽比（**Aspect ratio**）。GIoU考虑到了重叠面积的问题，DIoU考虑到了重叠面积和中心点距离的问题，CIoU则在此基础上进一步的考虑到了高宽比的问题。

![](https://pic1.zhimg.com/v2-5ab3e54940cc4266825fe03696950d98_b.jpg)

        CIoU的计算公式如下所示，可以看出，其在DIoU的基础上加多了一个惩罚项αv。其中α为权重为正数的重叠面积平衡因子，在回归中被赋与更高的优先级，特别是在两个边框不重叠的情况下；而v则用于测量宽高比的一致性。

<img src="https://pic4.zhimg.com/v2-c79bcf902d5454ca230cc255f821168b_b.jpg" title="" alt="" data-align="center">

## 1.8 F-EIoU Loss

        Focal and Efficient IoU Loss是由华南理工大学学者最近提出的一篇关于目标检测损失函数的论文，文章主要的贡献是提升网络收敛速度和目标定位精度。目前检测任务的损失函数主要有两个缺点：(1)无法有效地描述边界框回归的目标，导致收敛速度慢以及回归结果不准确（2）忽略了边界框回归中不平衡的问题。

        F-EIou loss首先提出了一种有效的交并集（IOU）损失，它可以准确地测量边界框回归中的**重叠面积**、**中心点**和**边长**三个几何因素的差异：

<img src="https://pic1.zhimg.com/v2-c38f06a86f2f3457f139efddc73877f8_b.jpg" title="" alt="" data-align="center">

        其次，基于对有效样本挖掘问题（EEM）的探讨，提出了Focal loss的回归版本，以使回归过程中专注于高质量的锚框：

<img src="https://pic2.zhimg.com/v2-5d8121b6d7e82ab85258e4cee431de0d_b.jpg" title="" alt="" data-align="center">

        最后，将以上两个部分结合起来得到Focal-EIou Loss：

<img src="https://pic3.zhimg.com/v2-2f81e4a88f1c81356e3977799254e466_b.jpg" title="" alt="" data-align="center">

        其中，通过加入每个batch的权重和来避免网络在早期训练阶段收敛慢的问题。

## 1.9 CDIoU Loss

        Control Distance IoU Loss是由同济大学学者提出的，文章的主要贡献是在几乎不增强计算量的前提下有效提升了边界框回归的精准度。目前检测领域主要两大问题：（1）SOTA算法虽然有效但计算成本高（2）边界框回归损失函数设计不够合理。

<img src="https://pic2.zhimg.com/v2-36c6fec092e16dd148170177285a1c29_b.png" title="" alt="" data-align="center">

        文章首先提出了一种对于Region Propasl（RP）和Ground Truth（GT）之间的新评估方式，即CDIoU。可以发现，它虽然没有直接中心点距离和长宽比，但最终的计算结果是有反应出RP和GT的差异。计算公式如下：

<img src="https://pic1.zhimg.com/v2-6ea636288fe731bfc2119031ddc9c2b4_b.jpg" title="" alt="" data-align="center">

<img src="https://pic2.zhimg.com/v2-78ac5df80f27943c4964d34d514bcf91_b.jpg" title="" alt="" data-align="center">

        对比以往直接计算中心点距离或是形状相似性的损失函数，CDIoU能更合理地评估RP和GT的差异并且有效地降低计算成本。然后，根据上述的公式，CDIoU Loss可以定义为：

<img src="https://pic1.zhimg.com/v2-b17384c8fe49913da1f97c55ad176518_b.jpg" title="" alt="" data-align="center">

        通过观察这个公式，可以直观地感受到，在权重迭代过程中，模型不断地将RP的四个顶点拉向GT的四个顶点，直到它们重叠为止，如下图所示：

<img src="https://pic2.zhimg.com/v2-5335a65d3602b07dbab4bbf9e8e54709_b.jpg" title="" alt="" data-align="center">

## 1.10  Huber Loss

Huber Loss是一种将MSE与MAE结合起来，取两者优点的损失函数，也被称作Smooth Mean Absolute Error Loss 。其原理很简单，就是在误差接近0时使用MSE，误差较大时使用MAE，公式为：

![[公式]](https://www.zhihu.com/equation?tex=J_%7B%5Ctext+%7Bhuber%7D%7D%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cmathbb%7BI%7D_%7B%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5Cleq+%5Cdelta%7D+%5Cfrac%7B%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B2%7D%2B%5Cmathbb%7BI%7D_%7B%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C%3E%5Cdelta%7D%5Cleft%28%5Cdelta%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C-%5Cfrac%7B1%7D%7B2%7D+%5Cdelta%5E%7B2%7D%5Cright%29+%5C%5C)

上式中，![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta)是Huber Loss的一个超参数，![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta)的值是MSE与MAE两个损失连接的位置。

huber损失是平方损失和绝对损失的综合，它克服了平方损失和绝对损失的缺点，不仅使损失函数具有连续的导数，而且利用MSE梯度随误差减小的特性，可取得更精确的最小值。尽管huber损失对异常点具有更好的鲁棒性，但是，它不仅引入了额外的参数，而且选择合适的参数比较困难，这也增加了训练和调试的工作量。

下图为![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%3D1.0)时的Huber Loss：

![](https://pic4.zhimg.com/v2-eb08a578285c77bb55024da975995a8f_r.jpg)

可以看到在 ![[公式]](https://www.zhihu.com/equation?tex=%5B-%5Cdelta%2C+%5Cdelta%5D)内实际上就是MSE的损失，使损失函数可导并且梯度更加稳定；在![[公式]](https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C+%5Cdelta%29)和![[公式]](https://www.zhihu.com/equation?tex=%28%5Cdelta%2C+%5Cinfty%29)区间内为MAE损失，降低了异常点的影响，使训练更加鲁棒。

## 1.11  分位数损失（Quantile Loss

分位数回归Quantile Regression是一类在实际应用中非常有用的回归算法，通常的回归算法是拟合目标值的**期望（MSE）或者中位数（MAE）**，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数。例如我们可以分别拟合出多个分位点，得到一个置信区间，如下图所示：

<img src="https://pic2.zhimg.com/v2-79da46f00643fb3e8e11d76b2f995ab1_r.jpg" title="" alt="" data-align="center">

分位数回归是通过使用分位数损失Quantile Loss来实现这一点的，分位数损失形式如下：

![[公式]](https://www.zhihu.com/equation?tex=J_%7Bq+u+a+n+t%7D%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cmathbb%7BI%7D_%7B%5Chat%7By%7D_%7Bi%7D+%5Cgeq+y_%7Bi%7D%7D%281-r%29%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C%2B%5Cmathbb%7BI%7D_%7B%5Chat%7By%7D_%7Bi%7D%3Cy_%7Bi%7D%7D+r%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5C%5C)

式中的r为分位数，这个损失函数是一个分段的函数，将![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D+%5Cgeq+y_%7Bi%7D)（高估）和 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%3Cy_%7Bi%7D) （低估）时，低估的损失要比高估的损失更大；反之，当![[公式]](https://www.zhihu.com/equation?tex=r%3C0.5) 时，高估的损失要比低估的损失更大，分位数损失实现了**分别用不同的系数控制高估和低估的损失，进而实现分位数回归**。特别地，当![[公式]](https://www.zhihu.com/equation?tex=r%3D0.5)时，分位数损失退化为MAE损失，从这里可以看出 MAE 损失实际上是分位数损失的一个特例—中位数回归

![[公式]](https://www.zhihu.com/equation?tex=J_%7Bq+u+a+n+t%7D%5E%7Br%3D0.5%7D%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%7Cy_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%7C+%5C%5C)

## 1.12  指数损失

指数损失函数的标准形式如下：![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+f%28X%29%29%3D%5Cexp+%28-Y+f%28X%29%29)

Adaboost中使用了指数损失函数，李航书中证明了：Adaboost算法是前向分步算法的特例，模型是由基本分类器组成的**加法模型**，损失函数为**指数函数**

## 1.13  对数损失/对数似然损失（Log-likelihood Loss）

log对数损失函的标准形式如下：![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+P%28Y+%7C+X%29%29%3D-%5Clog+P%28Y+%7C+X%29)

log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合；健壮性不强，相比于hinge loss对噪声更敏感

## 2. 分类损失

## 2.1 Entropy

        即“熵”，**熵**的概念最早起源于物理学，用于度量一个热力学系统的**无序**程度。但更常见的，在信息论里面， 熵是用于描述对**不确定性**的度量。所以，这个概念可以延伸到深度神经网络中，比如我们的模型在做分类时，其实也是在做一个判断一个物体到底是不是属于某个类别。因此，在正式介绍分类损失函数时，我们必须先了解熵的概念。

        数字化时代，信息都是由Bits(0和1)组成的。在通信时，有些位是**有用（useful）**的信息，有些位则是**冗余（redundant）**的信息，有些位甚至是**错误（error）**的信息，等等。当我们传达信息时，我们希望尽可能多地向接收者传递有用的信息。

> 传输1比特的信息意味着将接收者的不确定性降低2倍。 —— 香浓

        下面以一个天气预报的例子为例，形象化的讲解熵到底尤为何物？假设一个地方的天气是随机的，每天有**50%**的机会是晴天或雨天。

![](https://pic3.zhimg.com/v2-db9a3f07378c578466b7effd06c4fe2a_b.jpg)

        现在，如果气象站告诉您明天将要下雨，那么他们将**不确定性**降低了2倍。起初，有两种同样可能的可能性，但是在收到气象站的更新信息后，我们只有一种 。 在这里，气象站向我们发送了一点**有用**的信息，无论他们如何编码这些信息，这都是事实。即使发送的消息是雨天的，每个字符占一个字节，消息的总大小为40位，但它们仍然**只通信1位的有用信息**。现在，我们假设天气有8种可能状态，且都是等可能的。

![](https://pic1.zhimg.com/v2-7fcd4321e6a328aeee2a20d99e0ef7c8_b.jpg)

        那么，当气象站为您提供第二天的天气时，它们会将您的不确定性降低了**8**倍。由于每个事件的发生几率为1/8，因此**降低因子**为8。但如果这些可能性不是等概率的呢？比如，75%的机会是晴天，25%的机会是雨天。

![](https://pic4.zhimg.com/v2-d453f27dc2779ae7ca5ba7b4018e76a3_b.jpg)

        现在，如果气象台说第二天会下雨，那么你的不确定性就降低了**4**倍，也就是**2比特**的信息。**不确定性的减少就是事件概率的倒数**。在这种情况下，25%的倒数是4，log(4)以2为底得到2。因此，我们得到了**2位有用的信息**。

![](https://pic1.zhimg.com/v2-9543c300064b308dcee321229d1f9e90_b.jpg)

        如果气象站说第二天是晴天，那么我们得到**0.41**比特的有用信息。那么，我们**平均**能从气象站得到多少信息呢？明天是晴天的概率是75%这就给了你0.41比特的信息而明天是雨天的概率是25%这就给了你2比特的信息，这就对应我们平均每天从气象站得到0.81比特的信息。

![](https://pic3.zhimg.com/v2-e51257f79720b90ae420cef1ac32eca6_b.jpg)

        我们刚刚所计算出来的就叫做熵，它可以很好地描述事件的不确定性。它是由以下公式给出：

![](https://pic3.zhimg.com/v2-15d2d57984d199555361d7c06527882e_b.png)

        它衡量的是你每天了解天气情况时所得到的**平均信息量**。一般来说，它给出了给定**概率分布p**中样本值的平均信息量它告诉我们概率分布有**多不可预测**。如果我们住在沙漠中央，那里每天都是阳光灿烂的，平均来说，我们不会每天从气象站得到很多信息。熵会接近于零。另一方面，如果天气变化很大，熵就会大得多。

        总的来说：一个事件的不确定性就越大，其信息量越大，它的熵值就越高。比如**CVHub**今日宣布上市。相反，如果一个时间的不确定性越小，其信息量越小，它的熵值就越低。比如**CVHub**今天又增加了一个读者。

## 2.2 Cross Entropy

        现在，让我们讨论一下**交叉熵**。它只是**平均信息长度**。考虑同样的例子，8种可能的天气条件，所有都是等可能的，每一种都可以用3位编码 [2^3=8]。

![](https://pic3.zhimg.com/v2-f27fe14235149cfd726d22ef4dfc6812_b.jpg)

        这里的**平均信息长度**是3，这就是**交叉熵**。但是现在，假设你住在一个阳光充足的地区，那里的天气概率分布是这样的：

![](https://pic4.zhimg.com/v2-eae52703712183ab4f0683f0f41e01a3_b.jpg)

        即每天有35%的机会出现晴天，只有1%的机会出现雷雨。我们可以计算这个概率分布的熵，我们得到**2.23bits的熵**，具体计算公式如下：

![](https://pic2.zhimg.com/v2-536b25825f134a3c2d9318bf9ebce0f1_b.png)

        所以，平均来说，气象站发送了3个比特，但接收者只得到**2.23**个比特**有用**的信息。但是，我们可以做得更好。例如，让我们像这样更改编码方式：

![](https://pic3.zhimg.com/v2-ea8432186557fc970fae7e03cafc36da_b.jpg)

        现在，我们只使用2位用于表示晴天或部分晴天，使用3位用于多云和大部分多云，使用4位用于表示中雨和小雨，使用5位用于大雨和雷暴。天气的编码方式是明确的，并且如果你链接多条消息，则只有一种方法可以解释位的顺序。 例如，01100只能表示部分晴天（01），然后是小雨（100）。 因此，如果我们计算该站每天发送的平均比特数，则可以得出：

![](https://pic4.zhimg.com/v2-cc12c43b09cfbcee057ed618bfff6fe7_b.png)

        我们将得到4.58位。大约是熵的两倍。 平均而言，该站发送4.58位，但只有2.23位对接收者有用。 每条消息发送的信息量是必要信息的两倍。 这是因为我们使用的编码对天气分布做出了一些隐含的假设。 例如，当我们在晴天使用2位消息时，我们隐式地预测晴天的概率为25％。以同样的方式，我们计算所有天气情况。

![](https://pic1.zhimg.com/v2-5a6d7254d3e980ce176b0bca4ba01974_b.jpg)

        分母中2的幂对应于用于传输消息的比特数。很明显，**预测分布q和真实分布p有很大不同**。现在我们可以把交叉熵表示成真实概率分布p的函数和预测概率分布q的函数：

![](https://pic2.zhimg.com/v2-69a15f4ff95b0326a5612440169e4c49_b.png)

注意，这里对数的底数为2。

### 2.2.1 二分类：

考虑二分类，在二分类中我们通常使用Sigmoid函数将模型的输出压缩到![[公式]](https://www.zhihu.com/equation?tex=%280%2C1%29)区间内，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D+%5Cin%280%2C1%29)，用来代表给定输入![[公式]](https://www.zhihu.com/equation?tex=%7Bx%7D_%7Bi%7D)，模型判断为正类的概率。由于只有正负两类，因此同时也得到了负类的概率：

![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D%3D1+%7C+x_%7Bi%7D%5Cright%29%3D%5Chat%7By%7D_%7Bi%7D+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D%3D0+%7C+x_%7Bi%7D%5Cright%29%3D1-%5Chat%7By%7D_%7Bi%7D+%5C%5C)

将两条式子合并成一条：

![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D+%7C+x_%7Bi%7D%5Cright%29%3D%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7By_%7Bi%7D%7D%5Cleft%281-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B1-y_%7Bi%7D%7D+%5C%5C)

假设数据点之间独立同分布，则似然可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=L%28x%2C+y%29%3D%5Cprod_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7By_%7Bi%7D%7D%5Cleft%281-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B1-y_%7Bi%7D%7D+%5C%5C)

对似然取对数，然后加负号变成最小化负对数似然，即为交叉熵损失函数的形式：

![[公式]](https://www.zhihu.com/equation?tex=N+L+L%28x%2C+y%29%3DJ_%7BC+E%7D%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+y_%7Bi%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29%2B%5Cleft%281-y_%7Bi%7D%5Cright%29+%5Clog+%5Cleft%281-%5Chat%7By%7D_%7Bi%7D%5Cright%29+%5C%5C)

下图是对二分类的交叉熵损失函数的可视化：

![](https://pic3.zhimg.com/v2-5b091cf7bb79f36a67394973a73319d2_r.jpg)

蓝线是目标值为0时输出不同输出的损失，黄线是目标值为1时的损失。可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长

### 2.2.2 多分类：

在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D)是一个One-hot 向量，同时模型输出的压缩由原来的Sigmoid函数换成Softmax函数。Softmax 函数将每个维度的输出范围都限定在![[公式]](https://www.zhihu.com/equation?tex=%280%2C1%29)之间，同时所有维度的输出和为1，用于表示一个概率分布

![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28y_%7Bi%7D+%7C+x_%7Bi%7D%5Cright%29%3D%5Cprod_%7Bk%3D1%7D%5E%7BK%7D%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5E%7Bk%7D%5Cright%29%5E%7By_%7Bi%7D%5E%7Bk%7D%7D+%5C%5C)

其中，![[公式]](https://www.zhihu.com/equation?tex=k+%5Cin+K)表示K个类别中的一类，同样的假设数据点之间独立同分布，可得到负对数似然为：

![[公式]](https://www.zhihu.com/equation?tex=N+L+L%28x%2C+y%29%3DJ_%7BC+E%7D%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D%5E%7Bk%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5E%7Bk%7D%5Cright%29+%5C%5C)

由于![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D)是一个One-hot向量，除了目标类为1之外其他类别上的输出都为 0，因此上式也可以写为：

![[公式]](https://www.zhihu.com/equation?tex=J_%7BC+E%7D%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+y_%7Bi%7D%5E%7Bc_%7Bi%7D%7D+%5Clog+%5Cleft%28y_%7Bi%7D%5E%7B%5Chat%7Bc%7D_%7Bi%7D%7D%5Cright%29+%5C%5C)

其中，![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D)是![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D)的目标类，通常这个应用于多分类的交叉熵损失函数也被称为Softmax Loss或者Categorical Cross Entropy Loss

### 2.2.3 Logistics loss和Cross Entropy Loss：

对于Logistics loss，我们说的是二分类问题，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)是一个数；对于Cross Entropy Loss，我们说的是多分类问题，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%7By%7D%7D)是一个k维的向量。当k=2时，Logistics loss与Cross Entropy Loss一致

### 2.2.4 为什么用交叉熵损失：

分类中为什么不用均方差损失？上文在介绍均方差损失的时候讲到实际上均方差损失假设了误差服从高斯分布，在分类任务下这个假设没办法被满足，因此效果会很差

为什么是交叉熵损失呢？

（1）一个角度是用最大似然来解释：也就是我们上面的推导

（2）另一个角度是用信息论来解释交叉熵损失：假设对于样本![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D) 存在一个最优分布![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%5E%7B%5Cstar%7D)真实地表明了这个样本属于各个类别的概率，那么我们希望模型的输出![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D)尽可能地逼近这个最优分布，在信息论中，我们可以**使用KL散度（Kullback–Leibler Divergence）来衡量两个分布的相似性**。给定分布![[公式]](https://www.zhihu.com/equation?tex=p)和分布![[公式]](https://www.zhihu.com/equation?tex=q)， 两者的 KL 散度公式如下：

![[公式]](https://www.zhihu.com/equation?tex=K+L%28p%2C+q%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D+p+%5Clog+%28p%29-%5Csum_%7Bk%3D1%7D%5E%7BK%7D+p+%5Clog+%28q%29+%5C%5C)

其中第一项为分布![[公式]](https://www.zhihu.com/equation?tex=p)的信息熵，第二项为分布![[公式]](https://www.zhihu.com/equation?tex=p)和分布![[公式]](https://www.zhihu.com/equation?tex=q)的交叉熵。将最优分布![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%5E%7B%5Cstar%7D)和输出分布![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D)代入分布![[公式]](https://www.zhihu.com/equation?tex=p)和分布![[公式]](https://www.zhihu.com/equation?tex=q)得到：

![[公式]](https://www.zhihu.com/equation?tex=K+L%5Cleft%28y_%7Bi%7D%5E%7B%5Cstar%7D%2C+%5Chat%7By%7D_%7Bi%7D%5Cright%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D%5E%7B%5Cstar%7D+%5Clog+%5Cleft%28y_%7Bi%7D%5E%7B%5Cstar%7D%5Cright%29-%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D%5E%7B%5Cstar%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29+%5C%5C)

由于我们希望两个分布尽量相近，因此我们最小化KL散度。同时由于上式第一项信息熵仅与最优分布本身相关，因此我们在最小化的过程中可以忽略掉，变成最小化

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D%5E%7B%5Cstar%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29+%5C%5C)

我们并不知道最优分布![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%5E%7B%5Cstar%7D)，但训练数据里面的目标值![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D) 可以看做是![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%5E%7B%5Cstar%7D)的一个近似分布：

![[公式]](https://www.zhihu.com/equation?tex=-%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5Cright%29+%5C%5C)

这个是针对单个训练样本的损失函数，如果考虑整个数据集，则：

![[公式]](https://www.zhihu.com/equation?tex=J_%7BK+L%7D%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Csum_%7Bk%3D1%7D%5E%7BK%7D+y_%7Bi%7D%5E%7Bk%7D+%5Clog+%5Cleft%28%5Chat%7By%7D_%7Bi%7D%5E%7Bk%7D%5Cright%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+y_%7Bi%7D%5E%7Bc_%7Bi%7D%7D+%5Clog+%5Cleft%28y_%7Bi%7D%5E%7B%5Chat%7Bc%7D_%7Bi%7D%7D%5Cright%29+%5C%5C)

可以看到**通过最小化交叉熵的角度推导出来的结果和使用最大化似然得到的结果是一致的**

（3）最后一个角度为BP过程：当使用平方误差损失函数时，最后一层的误差为![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D%3D-%5Cleft%28y-a%5E%7B%28l%29%7D%5Cright%29+f%5E%7B%5Cprime%7D%5Cleft%28z%5E%7B%28l%29%7D%5Cright%29)，其中最后一项为![[公式]](https://www.zhihu.com/equation?tex=f%5E%7B%5Cprime%7D%5Cleft%28z%5E%7B%28l%29%7D%5Cright%29)，为激活函数的导数。当激活函数为Sigmoid函数时，如果![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%29%7D)的值非常大，函数的梯度趋于饱和，即![[公式]](https://www.zhihu.com/equation?tex=f%5E%7B%5Cprime%7D%5Cleft%28z%5E%7B%28l%29%7D%5Cright%29)的绝对值非常小，导致![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D)的取值也非常小，使得基于梯度的学习速度非常缓慢；

当使用交叉熵损失函数时，最后一层的误差为![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D%3Df%5Cleft%28z_%7Bk%7D%5E%7B%28l%29%7D%5Cright%29-1%3Da_%7B%5Ctilde%7Bk%7D%7D%5E%7B%28l%29%7D-1)，此时导数是线性的，因此不存在学习速度过慢的问题

引入交叉熵损失函数目的是解决一些实例在刚开始训练时学习得非常慢的问题，其主要针对激活函数为Sigmod 函数，如果在输出神经元是S型神经元时，交叉熵一般都是更好的选择，交叉熵无法改善隐藏层中神经元发生的学习缓慢，交叉熵损失函数只对网络输出明显背离预期时发生的学习缓慢有改善效果， 交叉熵损失函数并不能改善或避免神经元饱和，而是当输出层神经元发生饱和时，能够避免其学习缓慢的问题。可参考

## 2.3 K-L Divergence

        即KL散度。对于交叉熵损失，除了我们在这里使用**预测概率的对数（log(q(i))）**外，它看起来与上面**熵**的方程非常相似。如果我们的预测是完美的，那就是**预测分布等于真实分布**，此时**交叉熵就等于熵**。 但是，如果分布不同，则**交叉熵将比熵大一些位数**。 **交叉熵超过熵的量称为相对熵**，或更普遍地称为**库尔贝克-莱布里埃发散度（KL Divergence**）。总结如下：

![](https://pic2.zhimg.com/v2-a8f394efee3eb20ad3fc6f465993072d_b.jpg)

        接上面的例子，我们便可以顺便算出： **KL散度 = 交叉熵 - 熵 = 4.58 - 2.23 = 2.35（Bits）**。通常来说，一般分类损失最常用的损失函数之一便是交叉熵损失。假设我们当前做一个3个类别的图像分类任务，如猫、狗、猪。给定一张输入图片其真实类别是猫，模型通过训练用Softmax分类后的输出结果为：｛"cat": 0.3, "dog": 0.45, "pig": 0.25｝，那么此时交叉熵为：-1 * log(0.3) = 1.203。当输出结果为：｛"cat": 0.5, "dog": 0.3, "pig": 0.2｝时，交叉熵为：-1 * log(0.5) = 0.301。可以发现，当真实类别的预测概率接近于0时，损失会变得非常大。但是当预测值接近真实值时，损失将接近0。

## 2.4 Dice Loss

        即骰子损失，出自V-Net [3]，是一种用于评估两个样本之间相似性度量的函数，取值范围为0~1，值越大表示两个值的相似度越高，其基本定义（二分类）如下：

![](https://pic3.zhimg.com/v2-4e9e4226d0777e200c6b63610c9db7be_b.jpg)

        其中，|X∩Y|表示X和Y之间的交集，|X|和|Y|分别表示集合X和Y中像素点的个数，分子乘于2保证域值范围在0~1之间，因为分母相加时会计算多一次重叠区间，如下图：

<img src="https://pic1.zhimg.com/v2-b629203738fe1c9ce5235663dc6c71d4_b.jpg" title="" alt="" data-align="center">

        从右边公式也可以看出，其实Dice系数是等价于F1分数的，优化Dice等价于优化F1值。此外，为了防止分母项为0，一般我们会在分子和分母处同时加入一个很小的数作为平滑系数，也称为拉普拉斯平滑项。Dice损失由以下两个主要特性：

- 有益于正负样本不均衡的情况，侧重于对前景的挖掘；
- 训练过程中，在有较多小目标的情况下容易出现振荡；
- 极端情况下会出现梯度饱和的情况。

所以一般来说，我们都会结合交叉熵损失或者其他分类损失一同进行优化。

## 2.5 Focal Loss

        焦点损失，出自何凯明的《Focal Loss for Dense Object Detection》[4]，出发点是解决目标检测领域中one-stage算法如YOLO系列算法准确率不高的问题。作者认为样本的**类别不均衡**（比如前景和背景）是导致这个问题的主要原因。比如在很多输入图片中，我们利用网格去划分小窗口，大多数的窗口是不包含目标的。如此一来，如果我们直接运用原始的交叉熵损失，那么负样本所占比例会非常大，主导梯度的优化方向，即网络会偏向于将前景预测为背景。即使我们可以使用OHEM（在线困难样本挖掘）算法来处理不均衡的问题，虽然其增加了误分类样本的权重，但也容易忽略掉易分类样本。而Focal loss则是聚焦于训练一个困难样本的稀疏集，通过直接在标准的交叉熵损失基础上做改进，引进了两个惩罚因子，来减少易分类样本的权重，使得模型在训练过程中更专注于困难样本。其基本定义如下：

![](https://pic1.zhimg.com/v2-78ec2c165b42da5dd8fd351189473c4c_b.png)

其中：

- 参数 α 和 (1-α) 分别用于控制正/负样本的比例，其取值范围为[0, 1]。α的取值一般可通过交叉验证来选择合适的值。
- 参数 γ 称为聚焦参数，其取值范围为[0, +∞)，目的是通过**减少易分类**样本的权重，从而使模型在训练时更专注于困难样本。 当 γ = 0 时，Focal Loss就退化为交叉熵损失，γ 越大，对易分类样本的惩罚力度就越大。

实验中，作者取（α=0.25，γ=0.2）的效果最好，具体还需要根据任务的情况调整。由此可见，应用Focal-loss也会引入多了两个超参数需要调整，而一般来说很需要经验才能调好。

## 2.6 Tversky loss

        Tversky loss，发表于CVPR 2018上的一篇《Tversky loss function for image segmentation using 3D fully convolutional deep networks》文章 [5]，是根据Tversky 等人于1997年发表的《Features of Similarity》文章 [6] 所提出的Tversky指数所改造的。Tversky系数主要用于描述两个特征（集合）之间的相似度，其定义如下：

![](https://pic1.zhimg.com/v2-c4476d95ccc5cf8af42f1981a8e6bbb0_b.png)

由上可知，它是结合了Dice系数（F1-score）以及Jaccard系数（IoU）的一种广义形式，如：

- 当 α = β = 0.5时，此时Tversky loss便退化为Dice系数（分子分母同乘于2）
- 当 α = β = 1时，此时Tversky loss便退化为Jaccard系数（交并比）

因此，我们只需控制 α 和 β 便可以控制**假阴性**和**假阳性**之间的平衡。比如在医学领域我们要检测肿瘤时，更多时候我们是希望Recall值（查全率，也称为灵敏度或召回率）更高，因为我们不希望说将肿瘤检测为非肿瘤，即假阴性。因此，我们可以通过增大 β 的取值，来提高网络对肿瘤检测的灵敏度。其中，α + β 的取值我们一般会令其1。

## 2.7  合页损失（Hinge Loss）

合页损失（Hinge Loss）是另外一种二分类损失函数，适用于 maximum-margin 的分类，支持向量机Support Vector Machine (SVM)模型的损失函数本质上就是Hinge Loss + L2正则化。合页损失的公式如下：

![[公式]](https://www.zhihu.com/equation?tex=J_%7Bh+i+n+g+e%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cmax+%5Cleft%280%2C1-%5Coperatorname%7Bsgn%7D%5Cleft%28y_%7Bi%7D%5Cright%29+%5Chat%7By%7D_%7Bi%7D%5Cright%29+%5C%5C)

下图是![[公式]](https://www.zhihu.com/equation?tex=y)为正类，即![[公式]](https://www.zhihu.com/equation?tex=%5Coperatorname%7Bsgn%7D%28y%29%3D1)时，不同输出的合页损失示意图：

<img src="https://pic4.zhimg.com/v2-9613b424107186ee8061723092088fcb_r.jpg" title="" alt="" data-align="center">

可以看到当![[公式]](https://www.zhihu.com/equation?tex=y)为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在![[公式]](https://www.zhihu.com/equation?tex=%280%2C1%29)区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。使用合页损失直觉上理解是要**找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类**

## 2.8 softmax损失函数

> 公式： ![[公式]](https://www.zhihu.com/equation?tex=L%28Y%7Cf%28x%29%29%3D-%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Blog%5Cfrac%7Be%5E%7Bf_%7BY_%7Bi%7D%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bc%7D%7Be%5E%7Bf_%7Bj%7D%7D%7D%7D%7D)

从标准形式上看，softmax损失函数应归到对数损失的范畴，在监督学习中，由于它被广泛使用，所以单独形成一个类别。softmax损失函数本质上是逻辑回归模型在多分类任务上的一种延伸，常作为CNN模型的损失函数。softmax损失函数的本质是将一个k维的任意实数向量x映射成另一个k维的实数向量，其中，输出向量中的每个元素的取值范围都是(0,1)，即softmax损失函数输出每个类别的预测概率。由于softmax损失函数具有类间可分性，被广泛用于分类、分割、人脸识别、图像自动标注和人脸验证等问题中，其特点是类间距离的优化效果非常好，但类内距离的优化效果比较差。

softmax损失函数具有类间可分性，在多分类和图像标注问题中，常用它解决特征分离问题。在基于卷积神经网络的分类问题中，一般使用softmax损失函数作为损失函数，但是softmax损失函数学习到的特征不具有足够的区分性，因此它常与对比损失或中心损失组合使用，以增强区分能力。

## 2.9 感知损失(perceptron loss)函数

**感知损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+-f%28x%29%29++%5C%5C)

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss**只要样本的判定类别正确的话，它就满意，不管其判定边界的距离**。它比Hinge loss简单，因为不是max-margin boundary，所以模**型的泛化能力没 hinge loss强**。

## 总结

总的来说，损失函数的形式千变万化，但追究溯源还是万变不离其宗。其本质便是给出一个能较全面合理的描述两个特征或集合之间的相似性度量或距离度量，针对某些特定的情况，如类别不平衡等，给予适当的惩罚因子进行权重的加减。大多数的损失都是基于最原始的损失一步步改进的，或提出更一般的形式，或提出更加具体实例化的形式。
