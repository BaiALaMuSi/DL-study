总体来说，一部分正则化方法使给模型施加额外的限制条件，例如限制参数值范围，有些会在目标函数中添加一些额外惩罚项，本质上也是希望限制参数值。有的时候，这些限制条件或惩罚项代表了特定的先验经验，有的时候是希望避免模型过于复杂。正则化常常会增加一些bias但同时会减少variance，好的正则化方法就是在能够显著减小variance的情况下又不显著地增加bias。下面来总结一下常见的几种正则化方法：

## 1.Parameter Norm Penalties

即在目标函数中添加对于参数的惩罚项以减小模型的capacity，即：

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28%5Ctheta+%3B+X%2Cy%29+%3D+J%28%5Ctheta+%3B+X%2Cy%29+%2B+%5Calpha+%5COmega+%28%5Ctheta%29)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+%5Cin+%5B0%2C+%5Cinfty%29) 是一个标志着惩罚项权重的超参数。当 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 为零时，正则项为零，![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)越大，则正则化越显著。

由于加入了关于参数大小的惩罚项，模型训练过程不仅仅是要减小训练集上的目标函数，而且也要保持惩罚项尽量小，常见的惩罚项有 ![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization和 ![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization。其中![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization是减小权重的大小，而![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization会使权重更稀疏，具体分析如下：

![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization也被称作**weight decay(权重衰减）**，通过增加 ![[公式]](https://www.zhihu.com/equation?tex=%5COmega+%28%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C_2%5E2) 的正则项，可以使权重大小减小。

为什么 ![[公式]](https://www.zhihu.com/equation?tex=L%5E2) 可以减小权重w的大小呢？我们可以先从每次梯度下降更新的大小来理解：

对于 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28w+%3B+X%2Cy%29+%3D+J%28w+%3B+X%2Cy%29+%2B%5Cfrac%7B%5Calpha%7D%7B2%7Dw%5ETw) ，

其梯度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_w%5Ctilde%7BJ%7D%28w+%3B+X%2Cy%29+%3D+%5Ctriangledown_wJ%28w+%3B+X%2Cy%29+%2B%5Calpha+w+) ，则每次更新法则为

![[公式]](https://www.zhihu.com/equation?tex=w%5Cleftarrow+%281-%5Cepsilon+%5Calpha%29w+-+%5Cepsilon+%5Ctriangledown_wJ%28w+%3B+X%2Cy%29)

可以看出，![[公式]](https://www.zhihu.com/equation?tex=L%5E2)的引入使得每次步进时权重有一个常数衰减。对于单次步进如此，那么最终的w结果呢？

我们可以将目标函数在使原目标函数最小的 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%2A+%3D+argmin_w+J%28w%29) 附近进行二次展开 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28w%29+%3D+J%28w%5E%2A%29+%2B+%5Calpha+w+%2B+%5Cfrac%7B1%7D%7B2%7D%28w-w%5E%2A%29%5ETH%28w-w%5E%2A%29) ，其中H为Hessian matrix(其定义可回顾[梯度下降算法——深度学习花书第四章数值计算](https://zhuanlan.zhihu.com/p/38644738)中二阶导数部分)

该函数的极小值应取在满足 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_w%5Ctilde%7BJ%7D%28w%29+%3D+%5Calpha+w%2B+H%28w-w%5E%2A%29+%3D+0) 的w，用 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 代表新的极值点，则

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D+%3D+%28H%2B%5Calpha+I%29%5E%7B-1%7DHw%5E%2A)

可以看出当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 接近零时， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 趋近于 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%2A) ，而对于 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 远离零的情况，我们可对 ![[公式]](https://www.zhihu.com/equation?tex=H) 作本征分解为 ![[公式]](https://www.zhihu.com/equation?tex=H%3DQ%5CLambda+Q%5ET) ，则

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D+%3D+Q%28%5CLambda%2B%5Calpha+I%29%5E%7B-1%7D%5CLambda+Q%5ETw%5E%2A)

可以看出 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 相对于 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%2A) 是沿着 ![[公式]](https://www.zhihu.com/equation?tex=H) 的本征矢量的轴线进行了尺度收缩，即沿着 ![[公式]](https://www.zhihu.com/equation?tex=i) 本征向量的方向收缩了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda+_i%7D%7B%5Clambda+_i+%2B+%5Calpha%7D) ，对于本征值较大的 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+_i%5Cgg+%5Calpha) 的方向，正则项作用很小，而对于 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+_i%5Cll+%5Calpha) 的方向，正则项作用显著， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_i) 趋向于零，如下图中所示

![](https://pic4.zhimg.com/v2-f9af84ab23750b71bc6074ac2bcc40af_r.jpg)

图中实心线为不含正则项的原目标函数的等高线，虚线为正则项的等高线，![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D)为新的目标函数的极值点。对于 ![[公式]](https://www.zhihu.com/equation?tex=w_1) 方向，Hessian matrix的本征值很小，原目标函数对于这一水平方向的移动不敏感，所以正则项可沿![[公式]](https://www.zhihu.com/equation?tex=w_1) 方向从原极值点 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%2A) 作出较大横移，使其趋于零，而对于![[公式]](https://www.zhihu.com/equation?tex=w_2) 方向，其曲率较大，本征值较大，目标函数对于这一方向的移动非常敏感，导致正则项不能很大的改变 ![[公式]](https://www.zhihu.com/equation?tex=w_2) 方向上的值。

而![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization则是加入了对于每一个元素的绝对值的求和惩罚项 ![[公式]](https://www.zhihu.com/equation?tex=%5COmega+%28%5Ctheta%29+%3D+%7C%7Cw%7C%7C_1+%3D+%5Csum+_i+%7Cw_i%7C) ，新的目标函数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28w+%3B+X%2Cy%29+%3D+J%28w+%3B+X%2Cy%29+%2B%5Calpha+%7C%7Cw%7C%7C_1) ，

其梯度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_w%5Ctilde%7BJ%7D%28w+%3B+X%2Cy%29+%3D+%5Ctriangledown_wJ%28w+%3B+X%2Cy%29+%2B%5Calpha+sign%28w%29+) ,其中sign为正负号函数。

类似于![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization的处理过程，我们同样可以对新的目标函数进行二次展开，为方便讨论，假设Hessian matrix是对角矩阵（例如我们对于特征先进行PCA使其对角化），则新的目标函数可近似为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28w%29+%3D+J%28w%5E%2A%29+%2B+%5Csum_i%5Cleft%5B+%5Cfrac%7B1%7D%7B2%7DH_%7Bi%2Ci%7D%28w_i-w_i%5E%2A%29%5E2%2B%5Calpha+%7Cw_i%7C+%5Cright%5D) ，其极值满足

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_i+%3D+sign%28w_i%5E%2A%29max%5Cleft%5C%7B+%7Cw_i%5E%2A%7C-%5Cfrac%7B%5Calpha%7D%7BH_%7Bi%2Ci%7D%7D%2C0+%5Cright%5C%7D)

我们考虑 ![[公式]](https://www.zhihu.com/equation?tex=w_i%5E%2A%3E0) 的情况，![[公式]](https://www.zhihu.com/equation?tex=w_i%5E%2A%3C0)可做类似分析，（1） ![[公式]](https://www.zhihu.com/equation?tex=w_i%5E%2A%5Cleq%5Cfrac%7B%5Calpha%7D%7BH_%7Bi%2Ci%7D%7D) ，则极值点为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_i+%3D+0) （2） ![[公式]](https://www.zhihu.com/equation?tex=w_i%5E%2A%3E%5Cfrac%7B%5Calpha%7D%7BH_%7Bi%2Ci%7D%7D) ，则 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_i+%3D+w_i%5E%2A-%5Cfrac%7B%5Calpha%7D%7BH_%7Bi%2Ci%7D%7D) 不为零，而仅仅移动了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha%7D%7BH_%7Bi%2Ci%7D%7D) 的距离。与 ![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization相对比， ![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization使权重更**稀疏(sparse)**，即使很多项最优值为零，而对应的 ![[公式]](https://www.zhihu.com/equation?tex=L%5E2) 仅是使权重更小而不为零。

![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization由于其稀疏化的性质常常被用来做特征选择(feature selection)，因为它可以使某些权重为零，说明相对应的特征可以安全地忽略掉，经典的LASSO(least absolute shrinkage and selection operator)就是利用了 ![[公式]](https://www.zhihu.com/equation?tex=L%5E1) penalty。

## 2.Dataset Augmentation

使机器学习模型效果更好的很自然的一种办法就是给它提供更多的训练数据，当然实际操作中，有时候训练集是有限的，我们可以制造一些假数据并添加入训练集中，当然这仅对某些机器学习问题适用，例如对于图像识别，我们可以平移图像，添加噪声，旋转，色调偏移等等，我们希望模型能够在这些变换或干扰不受影响保持预测的准确性，从而减小泛化误差。当然，我们要注意这些变换不能改变数据的原始标记，如对于识别数字问题，我们就不能对6和9进行180度旋转。

## 3.Multi-task learning

与Dataset augmentation类似，多任务学习也是希望令模型的参数能够进行很好的泛化，其原理是对多个目标共享模型的一部分（输入及某些中间的表示层），使其对于多个有关联的目标均有较好的效果，保证模型可以更好的推广。

![](https://pic3.zhimg.com/80/v2-f035fe9189e231802c11a019948baa62_720w.jpg)

## 4.Early stopping

通常对于较大的模型，我们会观察到训练集上的误差不断减小，但验证集上的误差会在某个点之后反而逐渐增大，这意味着为了减小泛化误差，我们可以在训练过程中不断的记录验证集上的误差及对应的模型参数，最终返回验证集上误差最小所对应的模型参数，这个简单直观的方法就是early stopping，由于其简单高效，在深度学习中得到了广泛应用。

![](https://pic1.zhimg.com/v2-c26115267606c29be9dd71ecd9769a68_r.jpg)

## 5.Sparse Representations

![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization是使参数更稀疏，同样的我们也可以通过增加对于表征层的norm penalty项使表征（隐藏层）更稀疏。即 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%28%5Ctheta+%3B+X%2Cy%29+%3D+J%28%5Ctheta+%3B+X%2Cy%29+%2B+%5Calpha+%5COmega+%28h%29)

## 6.Bagging and Ensemble

Bagging(全称是bootstrap aggregating)通过整合多个模型来减小泛化误差，其基本思想是多个模型对于训练集做出同样错误预测的概率较小，Bagging是ensemble methods（集成算法）的一种方法。

例如假设我们有k个回归模型的集合，对于一个数据，每个模型的误差是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+_i) ，假设误差满足多元正太分布且方差 ![[公式]](https://www.zhihu.com/equation?tex=E%5B%5Cepsilon+_i%5E2%5D+%3D+v) ，协方差 ![[公式]](https://www.zhihu.com/equation?tex=E%5B%5Cepsilon_i+%5Cepsilon_j%5D%3Dc) ，则ensemble之后的误差平方的期望为

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%5B+%5Cleft%28+%5Cfrac%7B1%7D%7Bk%7D+%5Csum_i+%5Cepsilon+_i+%5Cright%29%5E2+%5Cright%5D+%3D+%5Cfrac%7B1%7D%7Bk%5E2%7DE%5Cleft%5B+%5Cleft%28+%5Csum_i+%5Cepsilon+_i%5E2%2B%5Csum_%7Bj%5Cne+i%7D%5Cepsilon_i+%5Cepsilon_j+%5Cright%29+%5Cright%5D+%3D+%5Cfrac%7B1%7D%7Bk%7Dv+%2B+%5Cfrac%7Bk-1%7D%7Bk%7Dc)

当误差间完全相关时，则 ![[公式]](https://www.zhihu.com/equation?tex=c%3Dv) ，均方差仍为 ![[公式]](https://www.zhihu.com/equation?tex=v) ，ensemble并没有达到更好效果，但如果误差间完全不相关，则 ![[公式]](https://www.zhihu.com/equation?tex=c%3D0) ，则均方差缩小至 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bk%7Dv) ,说明均方差随ensemble的集合大小而线性减小，实际应用中，介于这两种情况之间，通常ensemble效果会远好于其中单一模型的效果。

ensemble方法可以是将完全不同的模型结合起来，而Bagging方法则是同一种模型和目标函数，但是产生k种不同的训练集，每个训练集与原训练集所含数据量相同，但会以某一概率去掉某些样本并以其他的重复样本代替，第i个模型就在第i个训练集上进行训练。例如下图中所示，我们想鉴别某个数字是否是8，原数据包含9，6和8，第一个重新取样的数据集中仅包含6和8，9倍替换掉，则模型1会学习到需要数字有上半圈才能为8，而第二个重新取样的数据集中去掉6，仅包含9和8，则模型2会学习到数字需有下半圈才能为8，将他们集合起来就可以比较准确的推测需要同时有上下半圈才鉴定数字为8。

![](https://pic3.zhimg.com/v2-c9c951e3d1119f6788e1241db711ca36_r.jpg)

实际上，很多机器学习竞赛的优胜者都利用了ensemble的方法，但实际生产应用中，由于资源或速度的限制采用相对较少。

## 7.Dropout

Dropout可以理解做是将ensemble应用到大型神经网络的一种更为实际有效的方法。由于ensemble需要训练多个模型，对于大型神经网络，其训练和评估所需时间和存储资源较大，这种方法常常不太实际，Dropout就提供了一个更便宜的解决方案：即通过随机去掉一些节点的方法训练多个子网络，并最终将这些子网络ensemble起来，如下图所示：

![](https://pic2.zhimg.com/v2-48f5db9a50a08e6867373bbbfc70e49d_r.jpg)

其具体方法是当我们利用minibatch的算法如随机梯度下降算法来学习时，我们可以随机的选取一个binary mask(0表示节点输出为零，1表示正常输出该节点）决定哪些输入和隐藏层节点保留，每次的mask的选择是独立的。而mask为1的概率是我们可以调控的超参数。

和bagging方法相比，bagging中每个模型是完全独立的，而dropout中，模型间由于继承了父网络中的参数的子集会共享一些参数，这使得在有限的存储空间中我们可以表示多个模型。

以上是训练过程，而在做inference预测时，我们需要取所有模型的预测的均值，但是这往往计算量过多，Hinton提出inference时我们实际可以只用一个模型但其中每个节点的权重需要乘以包含这个节点的概率，这种方法称作weight scaling inference rule。实际中，我们常常把weight scaling过程放在训练过程中，即训练中每个节点输出就乘以包含该节点的概率的倒数，则inference时只需要正常的通过一遍前馈过程即可，不需要在进行weight scaling。

Dropout的优势在于其计算资源占用小，并且对于模型或训练算法的限制较小，基本上可以适用于各种前馈网络，循环网络或概率模型，所以实际工业模型中应用很多。

## 8.Adversarial Training

Adversarial Training对抗训练很有意思，它让人们深入思考机器学习究竟学到了什么有效信息。这方面的工作主要是由谷歌的Szegedy和本书作者Ian Goodfellow进行的。他们可以制造一些对抗样本迷惑神经网络，如下图中所示，他们对于熊猫图片加了一些人眼不可见的干扰，形成新样本，而新的人眼仍可鉴定为熊猫的图片却会被机器以较大置信率鉴定为长臂猿。

![](https://pic4.zhimg.com/v2-b4255b94a1e8ba0769227a37dff73d17_r.jpg)

为什么在人类看来类似的样本机器会得到大相径庭的结论呢？Ian认为这是由于神经网络中的大部分组成还是线性的(如ReLU可以看成是分段线性)，而对于不同的输入，线性函数会受到较大的扰动，产生较大的改变。为了解决这一问题，他们会将这些对抗样本重新加到训练集中，使得神经网络倾向于对于数据集保持局部稳定而不至干扰过大，从而学习到更有效的信息。

## 总结

至此，第七章正则化方法总结完毕，这章知识对于应用神经网络处理实际问题很有必要，而且某些方法可以结合起来一起使用。由于自己平时做用户推荐算法，Dataset Augmentation和Adversarial Training用的不多，但其他方法![[公式]](https://www.zhihu.com/equation?tex=L%5E2) regularization和 ![[公式]](https://www.zhihu.com/equation?tex=L%5E1) regularization，Early stopping, Dropout等用的比较多，multi-task learning也有涉及，而对于要参加机器学习竞赛如kaggle等比赛的同学可能ensemble方法会比较多的用到，实际工业界则可能相对较少，具体问题还得多多尝试。
