# 1.为什么需要 Normalization

1.**独立同分布与白化**

        机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为，莫过于“**独立同分布**”了，即*independent and identically distributed*，简称为 *i.i.d.* 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。

        因此，在把数据喂给机器学习模型之前，“**白化（whitening）**”是一个重要的数据预处理步骤。白化一般包含两个目的：

    （1）*去除特征之间的相关性* —> 独立；

    （2）*使得所有特征具有相同的均值和方差* —> 同分布。

2.**深度学习中的 Internal Covariate Shift**

        深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。

    Google 将这一现象总结为 Internal Covariate Shift，简称 ICS. 什么是 ICS 呢？

 下面做出了一个很好的解释：

> 大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有![[公式]](https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D),![[公式]](https://www.zhihu.com/equation?tex=P_s%28Y%7CX%3Dx%29%3DP_t%28Y%7CX%3Dx%29%5C%5C)但是![[公式]](https://www.zhihu.com/equation?tex=P_s%28X%29%5Cne+P_t%28X%29%5C%5C)大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。

3. **ICS 会导致什么问题？**

        简而言之，每个神经元的输入数据不再是“独立同分布”。

        其一，上层参数需要不断适应新的输入数据分布，降低学习速度。

        其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。

        其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

# 2.Normalization 的通用框架与基本思想

        我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D%3D%28x_1%2C+x_2%2C+%5Ccdots%2C+x_d%29%5C%5C) 通过某种运算后，输出一个标量值：

![[公式]](https://www.zhihu.com/equation?tex=y%3Df%28%5Cbold%7Bx%7D%29%5C%5C)

        由于 ICS 问题的存在， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。

        因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 送给神经元之前，先对其做**平移和伸缩变换**， 将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的分布规范化成在固定区间范围的标准分布。

        通用变换框架就如下所示：

![[公式]](https://www.zhihu.com/equation?tex=h%3Df%5Cleft%28%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%2B%5Cbold%7Bb%7D%5Cright%29%5C%5C)

        我们来看看这个公式中的各个参数。

        （1） ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D) 是**平移参数**（shift parameter）， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D) 是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D%3D%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%5C%5C) 得到的数据符合均值为 0、方差为 1 的标准分布。

        （2） ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 是**再平移参数**（re-shift parameter）， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D) 是**再缩放参数**（re-scale parameter）。将 上一步得到的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D) 进一步变换为： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7By%7D%3D%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+%2B+%5Cbold%7Bb%7D%5C%5C)

最终得到的数据符合均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) 的分布。

        奇不奇怪？奇不奇怪？说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？答案是——**为了保证模型的表达能力不因为规范化而下降**。我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。沮不沮丧？沮不沮丧？难道我们底层神经元人民就在做无用功吗？所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。

        除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。

        那么问题又来了——**经过这么的变回来再变过去，会不会跟没变一样？**

不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的均值取决于下层神经网络的复杂关联；但在新参数中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7By%7D%3D%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+%2B+%5Cbold%7Bb%7D) 仅由 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。

那么还有一个问题——**这样的 Normalization 离标准的白化还有多远？** 标准白化操作的目的是“独立同分布”。独立就不说了，暂不考虑。变换为均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）

# 3.主流Normalization

## **3.1 Batch Normalization**

**3.1.1 动机：**

        试想，数据在经过深度神经模型的某隐层之后，是否还保持着输入该隐层之前的数据分布？显然，难以避免会有细微的偏差。深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们被迫非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。但当网络层数很大时，仍然无法训练得到泛化能力很好的网络模型。Google 将这一现象总结为 **Internal Covariate Shift**。batch normalization很好地解决了这一问题。其思想是在每一层神经网络之后，对数据重新进行标准化（normalization），使数据分布恢复到尽量接近原分布。这里的“batch”是指模型在训练时，每次输入一个batch，只对这一个batch进行标准化。当然在具体实现时，也可以保留着以往batch的聚合值（均值和方差），从而尽可能地达到全局标准化。

**3.1.2 原理：**

<img src="https://pic1.zhimg.com/80/v2-b486bfc224df128cbdaf5b75fc7c2b7c_720w.jpg" title="" alt="" data-align="center">

        Batch Normalization 于2015年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 的均值和方差,因而称为 Batch Normalization。

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i+%3D+%5Cfrac%7B1%7D%7BM%7D%5Csum%7Bx_i%7D%2C+%5Cquad+%5Csigma_i%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7BM%7D%5Csum%7B%28x_i-%5Cmu_i%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=M) 是 mini-batch 的大小。

        按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。BN 独立地规范化每一个输入维度 ![[公式]](https://www.zhihu.com/equation?tex=x_i) ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。

        因此，BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。

        在使用BN层时，需要的假设是每个mini batch应该是同分布（或者近似同分布）的，如果不同mini batch的分布差异较大，相当于这个BN层需要学习不同的变换，这便无法解决上述提到的ICS问题。因此，在使用BN层时，batchsize尽可能调大、且数据集彻底打乱，否则BN的效果会显著变差。显而易见，BN也并不适用于需要先后输入数据的RNN模型。

**3.1.3 附录**

      （1）实现步骤：

1.  BN的计算就是把每个通道C的NHW单独拿出来标准化处理+缩放和平移

      （2）BN的使用位置:

全连接层或卷积层之后,激活层之前

      （3）BN的优缺点:

1. BN层已经上成为了标配。但是BN层在训练过程中需要在batch上计算中间统计量，这使得BN层严重依赖batch,如果一堆数据集,每次取得的batch size很小,那么计算出来的均值和方差不足以代表整个数据的分布,往往会恶化性能
2. 但是batch size很大的时候,会超过内存容量。
3. 依赖于batch_size的大小，batch_size越小，效果越不好。
4. 解决数据在经过深度神经网络层之后产生的内部协变量偏移问题，进而加速训练，更易收敛。
5. 对于RNN等变长输入的模型，在运算到网络的尾部时，只有少量较长序列还在参与运算，退化为batch_size过小问题。

## **3.2 Layer Normalization**

**3.2.1 动机：**

batch normalization不适用于RNN等动态网络和batchsize较小的场景：

1. 当batch size太小时，比如一个batch只有2个样本，都很难称得上是个分布，如果进行batch normalization，恐怕每个batch的输出都一模一样了吧。
2. RNN等动态网络场景，其实本质原因与1一样；由于RNN共享隐层参数且输入序列是不定长的，RNN的时间片进行到尾部时，往往只有最长的那两三个序列还在运算，等同于batch size一直减小，直到为0。

layer normalization的出现很好的解决了上述问题。layer normalization是对每个样本进行标准化，与batch的大小无关。

**3.2.2 原理：**

<img src="https://pic3.zhimg.com/80/v2-51d1d67d3c3b31d5eb97c4408f89278a_720w.jpg" title="" alt="" data-align="center">

        层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Csum_i%7Bx_i%7D%2C+%5Cquad+%5Csigma%3D+%5Csqrt%7B%5Csum_i%7B%28x_i-%5Cmu%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=i) 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 均为标量（BN中是向量），所有输入共享一个规范化变换。

        LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

        但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

        显然，无论数据维度为何，LN计算特征在除开batch维度以外所有维度的均值和方差，完成归一化。可以避免因为batchsize值过小导致mini batch分布不同的问题，同时，也不存在保存running_mean，running_var的问题。LN适用于RNN等模型。

**3.2.3 附录**

（1）​实现步骤

1. LN的计算就是把每个CHW单独拿出来标准化处理+缩放和平移

（2）LN的使用位置:它不依赖于batch size和输入sequence的长度,因此可以用于batch size很小的时候和RNN中,LN的效果比较明显.

（3）LN的优缺点:

1. 不受batch size的影响。

2. 避免了batch norm受限于batch size大小的问题。

3. 每个样本单独进行标准化，将不同样本约束在同一分布内，增强模型泛化能力。

## **3.3 Weight Normalization**

**3.3.1 动机：**

        BN 和 LN 均将规范化应用于输入的特征数据 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) ，这就是 WN 名称的来源。

**3.3.2 原理：**

<img src="https://pic2.zhimg.com/v2-93d904e4fff751a0e5b940ab3c27b6d5_r.jpg" title="" alt="" data-align="center">

        具体而言，WN 提出的方案是，**将权重向量** ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) **分解为向量方向** ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbold%7Bv%7D%7D) **和向量模** ![[公式]](https://www.zhihu.com/equation?tex=g) **两部分**：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cbold%7Bw%7D%7D+%3D+g%5Ccdot%5Chat%7B%5Cbold%7Bv%7D%7D+%3D+g%5Ccdot%5Cfrac%7B%5Cbold%7Bv%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bv%7D) 是与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 同维度的向量， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cv%7C%7C%7D)是欧氏范数，因此![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbold%7Bv%7D%7D) 是单位向量，决定了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的方向；![[公式]](https://www.zhihu.com/equation?tex=g) 是标量，决定了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的长度。由于 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7C%7B%5Cbold%7Bw%7D%7D%7C%7C+%5Cequiv+%7Cg%7C) ，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。

        **乍一看，这一方法似乎脱离了我们前文所讲的通用框架？** 并没有。其实从最终实现的效果来看，异曲同工。我们来推导一下看。 ![[公式]](https://www.zhihu.com/equation?tex=+f_%5Cbold%7Bw%7D%28WN%28%5Cbold%7Bx%7D%29%29%3D%5Cbold%7Bw%7D%5Ccdot+WN%28%5Cbold%7Bx%7D%29+%3D+g%5Ccdot%5Cfrac%7B%5Cbold%7Bv%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D+%5Ccdot%5Cbold%7Bx%7D+%5C%5C%3D+%5Cbold%7Bv%7D%5Ccdot+g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%3Df_%5Cbold%7Bv%7D%28g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%29)

对照一下前述框架：

![[公式]](https://www.zhihu.com/equation?tex=h%3Df%5Cleft%28%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%2B%5Cbold%7Bb%7D%5Cright%29%5C%5C)

我们只需令：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D+%3D+%5Cbold%7B%7C%7Cv%7C%7C%7D%2C+%5Cquad+%5Cbold%7B%5Cmu%7D%3D0%2C+%5Cquad+%5Cbold%7Bb%7D%3D0%5C%5C)

就完美地对号入座了！

        回忆一下，BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。**虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。**

        另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D%3D0). 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D) 。

**3.3.3 附录**

          WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

- WN是通过重写深度网络的权重来进行加速的，没有引入对minibatch的依赖，更适合于RNN网络。
- 引入更少的噪声。
- 不需要额外的空间进行存储minibatch的均值和方差，对时间的开销也小，所以速度会更快。

## 3.4 Instance Normalization

**3.4.1 动机：**

        作者指出在图像风格迁移任务中，生成式模型计算得到的 Feature Map 的各个 Channel 的均值与方差将影响到所生成图像的风格。故作者提出了 IN，在 Channel 层面对输入数据进行归一化，再使用目标图像的Channel 的均值与方差对结果进行 ‘去归一化’。一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。

**3.4.2 原理：**

<img src="https://pic2.zhimg.com/80/v2-e7f5074c1660dd1a9b4bcb029afcea95_720w.jpg" title="" alt="" data-align="center">

        前面提到，BN解决了模型ICS的问题，其实是将一个mini batch的分布重新映射了一遍，即基于batch的domain adaptation。但在Style Transfer任务中，模型更加关注每一张的特征（Style Transfer需要重构每张图的结构，而不是在一个batch内提取某些共有的feature）。因此，在Style Transfer中，每一张图片都是一个domain。针对每个feature map，我们在H,W维度将它其归一化，这便是IN。

<img src="https://pic3.zhimg.com/80/v2-6a5eb99a8ff1b55c98297f903d0bcfe6_720w.jpg" title="" alt="" data-align="center">

        由于很多Style Transfer任务仅仅是改变像素位置的颜色，因此，我们将通道维度也排除在归一化的范围外，这也是IN和LN的主要区别。其次，IN一般不会使用再放缩参数(affine = False)。和BN一样，IN需要计算running_mean, running_var。

        目前的结论是：在图片分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。

**3.4.3 附录**

​（1）实现步骤

1. IN的计算就是把每个HW单独拿出来标准化处理+缩放和平移

（2）IN的使用位置:

保持了每个图像的独立性,最初用于图像的风格化迁移

（3）IN的优点:

- 不受通道和batchsize 的影响

- 在图片视频分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。

## 3.5 Group Normalization

**3.5.1 动机：**

GN的动机也是为了解决BN受限于batch size的问题。

**3.5.2 原理：**

<img src="https://pic2.zhimg.com/80/v2-aa7deb543ede86de9cf954ae41315935_720w.jpg" title="" alt="" data-align="center">

        回到BN，实验表明，BN在batchsize较大的时候效果也得到提升（batchsize通常大于32），但是在大多数情况下，由于计算资源有限，我们的训练方案无法加载大的batchsize。这时，Group Normalization(GN)成为了一个解决方案。

GN将特征在通道维度（C）划分成若干个Group，然后对每个Group单独做normalization操作。对一个大小为 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+C+%5Ctimes+H+%5Ctimes+W) 的特征图，将它在 ![[公式]](https://www.zhihu.com/equation?tex=C) 维度分成 ![[公式]](https://www.zhihu.com/equation?tex=G) 组（ ![[公式]](https://www.zhihu.com/equation?tex=G) 为超参数）。显然，IN和LN分别是是 ![[公式]](https://www.zhihu.com/equation?tex=G%3DC) 的和 ![[公式]](https://www.zhihu.com/equation?tex=G%3D1) 特殊情况。在弥补了BN依赖大batchsize的缺点后，GN也集合了IN和LN的优势，一方面它可以学到channel维度不同位置的分布信息（IN），同时，他也能关注到channel之间的分布的关系（LN）。

**3.5.3 附录**

（1）实现步骤

1. 把C分成G组,每一组有C/G个通道,各组通道分别标准化+缩放和平移

（2）GN的优缺点:

1. 不受batch size 的影响,batch size很小的时候,GN的效果会比BN好.
2. GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C
3. 但对于大batch size，GN仍然难以匹敌BN。

## 3.6 **Switchable Normalization**

**3.6.1 原理**

将 BN、LN、IN 结合，赋予不同的权重，让网络自己去学习这些权重

![](https://pic4.zhimg.com/v2-4a783611e03d1272fb28fa3a5892951b_r.jpg)

![](https://pic4.zhimg.com/v2-09dcabe7a5a99cd2ddea11b76bad65af_r.jpg)

![](https://pic3.zhimg.com/v2-bcdd6a3a7e077a45cd42d21dd18e0686_r.jpg)

**3.6.2 附录**

缺点:训练复杂

## 7.1 Filter Response Normalization

**7.1.1 原理**

        谷歌的提出的FRN层包括归一化层FRN（Filter Response Normalization）和激活层TLU（Thresholded Linear Unit）,如图所示：

![](https://pic1.zhimg.com/v2-47c07c7138c696f2e5debd8cfed22614_r.jpg)

![](https://pic1.zhimg.com/v2-9dfbfcfe160491a5d538fa61315c4c5c_r.jpg)

![](https://pic2.zhimg.com/v2-fbdb63fd53aea730b404acae942b4bd9_r.jpg)

        标准化之后同样需要进行缩放和平移变换.FRN缺少去均值的操作，这可能使得标准化的结果任意地偏移0，如果FRN之后是ReLU激活层，可能产生很多0值，这对于模型训练和性能是不利的。为了解决这个问题，FRN之后采用的阈值化的ReLU，即TLU：

![](https://pic4.zhimg.com/v2-73f93800445177da6e5190495e738e93_r.jpg)

        ​这里的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 是一个可学习的参数。论文中发现FRN之后采用TLU对于提升性能是至关重要的.

**7.1.2 附录**

FRN的优缺点:

1. FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。
2. ​可以看到FRN是不受batch size的影响，而且效果是超越BN的。论文中还有更多的对比试验证明FRN的优越性。
3. BN目前依然是最常用的标准化方法，GN虽然不会受batch size的影响，但是目前还没大范围采用，不知道FRN的提出会不会替代BN，这需要时间的检验。

# 3.8 Cosine Normalization

**3.8.1 动机：**

        对输入数据$\mathbf{x}$的变换已经做过了，横着来是 LN，纵着来是 BN。对模型参数W $\mathbf{W}$的变换也已经做过了，就是 WN。那还有什么可做的呢？大佬们盯上了神经网络运算过程中的**内积**计算，即$f(x)=\mathbf{w} \cdot \mathbf{x}$中的“$\cdot$”。

        在经过深层网络结构之后，可能数据分布的方差已经很大，从而内积计算的结果也会很大，进而导致梯度爆炸等一系列问题。IN的目的是将这个结果值标准化到一个特定的区间

**3.8.2 原理：**

        我们再来看看神经元的经典变换 ![[公式]](https://www.zhihu.com/equation?tex=f_%5Cbold%7Bw%7D%28%5Cbold%7Bx%7D%29%3D%5Cbold%7Bw%7D%5Ccdot%5Cbold%7Bx%7D).对输入数据 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的变换已经做过了，横着来是 LN，纵着来是 BN。对模型参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的变换也已经做过了，就是 WN。好像没啥可做的了。然而天才的研究员们盯上了中间的那个点，对，就是![[公式]](https://www.zhihu.com/equation?tex=%5Ccdot)

他们说，我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 特征数据向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的点积。向量点积是无界（unbounded）的啊！

        那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围，多么的美好！仿佛看到了新的世界！于是，Cosine Normalization 就出世了。他们不处理权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) ，也不处理特征数据向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) ，就改了一下线性变换的函数：

![[公式]](https://www.zhihu.com/equation?tex=f_%5Cbold%7Bw%7D%28%5Cbold%7Bx%7D%29%3Dcos+%5Ctheta+%3D+%5Cfrac%7B%5Cbold%7Bw%7D%5Ccdot%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cw%7C%7C%7D%5Ccdot%5Cbold%7B%7C%7Cx%7C%7C%7D%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的夹角。然后就没有然后了，所有的数据就都是 [-1, 1] 区间范围之内的了！

        不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的内积，而分母则可以看做用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 二者的模之积进行规范化。对比一下 WN 的公式：

![[公式]](https://www.zhihu.com/equation?tex=+f_%5Cbold%7Bw%7D%28WN%28%5Cbold%7Bx%7D%29%29%3Df_%5Cbold%7Bv%7D%28g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%29%5C%5C)

一定程度上可以理解为，WN 用 权重的模 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cv%7C%7C%7D) 对输入向量进行 scale，而 CN 在此基础上用输入向量的模 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cx%7C%7C%7D) 对输入向量进行了进一步的 scale.

        CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。

# 3.9 总结

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201215113941183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0F1Z3VzdE1l,size_16,color_FFFFFF,t_70)
