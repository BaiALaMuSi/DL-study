# 1.为什么需要 Normalization

1.**独立同分布与白化**

        机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为，莫过于“**独立同分布**”了，即*independent and identically distributed*，简称为 *i.i.d.* 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。

        因此，在把数据喂给机器学习模型之前，“**白化（whitening）**”是一个重要的数据预处理步骤。白化一般包含两个目的：

    （1）*去除特征之间的相关性* —> 独立；

    （2）*使得所有特征具有相同的均值和方差* —> 同分布。

2.**深度学习中的 Internal Covariate Shift**

        深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。

    Google 将这一现象总结为 Internal Covariate Shift，简称 ICS. 什么是 ICS 呢？

 下面做出了一个很好的解释：

> 大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有![[公式]](https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D),![[公式]](https://www.zhihu.com/equation?tex=P_s%28Y%7CX%3Dx%29%3DP_t%28Y%7CX%3Dx%29%5C%5C)但是![[公式]](https://www.zhihu.com/equation?tex=P_s%28X%29%5Cne+P_t%28X%29%5C%5C)大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。

3. **ICS 会导致什么问题？**

        简而言之，每个神经元的输入数据不再是“独立同分布”。

        其一，上层参数需要不断适应新的输入数据分布，降低学习速度。

        其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。

        其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

# 2.Normalization 的通用框架与基本思想

        我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D%3D%28x_1%2C+x_2%2C+%5Ccdots%2C+x_d%29%5C%5C) 通过某种运算后，输出一个标量值：

![[公式]](https://www.zhihu.com/equation?tex=y%3Df%28%5Cbold%7Bx%7D%29%5C%5C)

        由于 ICS 问题的存在， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。

        因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 送给神经元之前，先对其做**平移和伸缩变换**， 将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的分布规范化成在固定区间范围的标准分布。

        通用变换框架就如下所示：

![[公式]](https://www.zhihu.com/equation?tex=h%3Df%5Cleft%28%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%2B%5Cbold%7Bb%7D%5Cright%29%5C%5C)

        我们来看看这个公式中的各个参数。

        （1） ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D) 是**平移参数**（shift parameter）， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D) 是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D%3D%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%5C%5C) 得到的数据符合均值为 0、方差为 1 的标准分布。

        （2） ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 是**再平移参数**（re-shift parameter）， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D) 是**再缩放参数**（re-scale parameter）。将 上一步得到的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D) 进一步变换为： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7By%7D%3D%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+%2B+%5Cbold%7Bb%7D%5C%5C)

最终得到的数据符合均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) 的分布。

        奇不奇怪？奇不奇怪？说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？答案是——**为了保证模型的表达能力不因为规范化而下降**。我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。沮不沮丧？沮不沮丧？难道我们底层神经元人民就在做无用功吗？所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。

        除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。

        那么问题又来了——**经过这么的变回来再变过去，会不会跟没变一样？**

不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的均值取决于下层神经网络的复杂关联；但在新参数中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7By%7D%3D%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+%2B+%5Cbold%7Bb%7D) 仅由 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。

那么还有一个问题——**这样的 Normalization 离标准的白化还有多远？** 标准白化操作的目的是“独立同分布”。独立就不说了，暂不考虑。变换为均值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 、方差为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2) 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）

# 3.主流Normalization

## **3.1 Batch Normalization**

**3.1.1 动机：**

        试想，数据在经过深度神经模型的某隐层之后，是否还保持着输入该隐层之前的数据分布？显然，难以避免会有细微的偏差。深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们被迫非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。但当网络层数很大时，仍然无法训练得到泛化能力很好的网络模型。Google 将这一现象总结为 **Internal Covariate Shift**。batch normalization很好地解决了这一问题。其思想是在每一层神经网络之后，对数据重新进行标准化（normalization），使数据分布恢复到尽量接近原分布。这里的“batch”是指模型在训练时，每次输入一个batch，只对这一个batch进行标准化。当然在具体实现时，也可以保留着以往batch的聚合值（均值和方差），从而尽可能地达到全局标准化。

**3.1.2 原理：**

<img src="https://pic1.zhimg.com/80/v2-b486bfc224df128cbdaf5b75fc7c2b7c_720w.jpg" title="" alt="" data-align="center">

        Batch Normalization 于2015年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 的均值和方差,因而称为 Batch Normalization。

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i+%3D+%5Cfrac%7B1%7D%7BM%7D%5Csum%7Bx_i%7D%2C+%5Cquad+%5Csigma_i%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7BM%7D%5Csum%7B%28x_i-%5Cmu_i%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=M) 是 mini-batch 的大小。

        按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。BN 独立地规范化每一个输入维度 ![[公式]](https://www.zhihu.com/equation?tex=x_i) ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。

        因此，BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。

        在使用BN层时，需要的假设是每个mini batch应该是同分布（或者近似同分布）的，如果不同mini batch的分布差异较大，相当于这个BN层需要学习不同的变换，这便无法解决上述提到的ICS问题。因此，在使用BN层时，batchsize尽可能调大、且数据集彻底打乱，否则BN的效果会显著变差。显而易见，BN也并不适用于需要先后输入数据的RNN模型。

**3.1.3 附录**

      （1）实现步骤：

1.  BN的计算就是把每个通道C的NHW单独拿出来标准化处理+缩放和平移

      （2）BN的使用位置:

全连接层或卷积层之后,激活层之前

      （3）BN的优缺点:

1. BN层已经上成为了标配。但是BN层在训练过程中需要在batch上计算中间统计量，这使得BN层严重依赖batch,如果一堆数据集,每次取得的batch size很小,那么计算出来的均值和方差不足以代表整个数据的分布,往往会恶化性能
2. 但是batch size很大的时候,会超过内存容量。
3. 依赖于batch_size的大小，batch_size越小，效果越不好。
4. 解决数据在经过深度神经网络层之后产生的内部协变量偏移问题，进而加速训练，更易收敛。
5. 对于RNN等变长输入的模型，在运算到网络的尾部时，只有少量较长序列还在参与运算，退化为batch_size过小问题。

## **3.2 Layer Normalization**

**3.2.1 动机：**

batch normalization不适用于RNN等动态网络和batchsize较小的场景：

1. 当batch size太小时，比如一个batch只有2个样本，都很难称得上是个分布，如果进行batch normalization，恐怕每个batch的输出都一模一样了吧。
2. RNN等动态网络场景，其实本质原因与1一样；由于RNN共享隐层参数且输入序列是不定长的，RNN的时间片进行到尾部时，往往只有最长的那两三个序列还在运算，等同于batch size一直减小，直到为0。

layer normalization的出现很好的解决了上述问题。layer normalization是对每个样本进行标准化，与batch的大小无关。

**3.2.2 原理：**

<img src="https://pic3.zhimg.com/80/v2-51d1d67d3c3b31d5eb97c4408f89278a_720w.jpg" title="" alt="" data-align="center">

        层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Csum_i%7Bx_i%7D%2C+%5Cquad+%5Csigma%3D+%5Csqrt%7B%5Csum_i%7B%28x_i-%5Cmu%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=i) 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 均为标量（BN中是向量），所有输入共享一个规范化变换。

        LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

        但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

        显然，无论数据维度为何，LN计算特征在除开batch维度以外所有维度的均值和方差，完成归一化。可以避免因为batchsize值过小导致mini batch分布不同的问题，同时，也不存在保存running_mean，running_var的问题。LN适用于RNN等模型。

**3.2.3 附录**

（1）​实现步骤

1. LN的计算就是把每个CHW单独拿出来标准化处理+缩放和平移

（2）LN的使用位置:它不依赖于batch size和输入sequence的长度,因此可以用于batch size很小的时候和RNN中,LN的效果比较明显.

（3）LN的优缺点:

1. 不受batch size的影响。

2. 避免了batch norm受限于batch size大小的问题。

3. 每个样本单独进行标准化，将不同样本约束在同一分布内，增强模型泛化能力。

## **3.3 Weight Normalization**

**3.3.1 动机：**

        BN 和 LN 均将规范化应用于输入的特征数据 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) ，这就是 WN 名称的来源。

**3.3.2 原理：**

<img src="https://pic2.zhimg.com/v2-93d904e4fff751a0e5b940ab3c27b6d5_r.jpg" title="" alt="" data-align="center">

        具体而言，WN 提出的方案是，**将权重向量** ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) **分解为向量方向** ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbold%7Bv%7D%7D) **和向量模** ![[公式]](https://www.zhihu.com/equation?tex=g) **两部分**：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cbold%7Bw%7D%7D+%3D+g%5Ccdot%5Chat%7B%5Cbold%7Bv%7D%7D+%3D+g%5Ccdot%5Cfrac%7B%5Cbold%7Bv%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bv%7D) 是与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 同维度的向量， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cv%7C%7C%7D)是欧氏范数，因此![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbold%7Bv%7D%7D) 是单位向量，决定了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的方向；![[公式]](https://www.zhihu.com/equation?tex=g) 是标量，决定了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的长度。由于 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7C%7B%5Cbold%7Bw%7D%7D%7C%7C+%5Cequiv+%7Cg%7C) ，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。

        **乍一看，这一方法似乎脱离了我们前文所讲的通用框架？** 并没有。其实从最终实现的效果来看，异曲同工。我们来推导一下看。 ![[公式]](https://www.zhihu.com/equation?tex=+f_%5Cbold%7Bw%7D%28WN%28%5Cbold%7Bx%7D%29%29%3D%5Cbold%7Bw%7D%5Ccdot+WN%28%5Cbold%7Bx%7D%29+%3D+g%5Ccdot%5Cfrac%7B%5Cbold%7Bv%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D+%5Ccdot%5Cbold%7Bx%7D+%5C%5C%3D+%5Cbold%7Bv%7D%5Ccdot+g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%3Df_%5Cbold%7Bv%7D%28g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%29)

对照一下前述框架：

![[公式]](https://www.zhihu.com/equation?tex=h%3Df%5Cleft%28%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%2B%5Cbold%7Bb%7D%5Cright%29%5C%5C)

我们只需令：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D+%3D+%5Cbold%7B%7C%7Cv%7C%7C%7D%2C+%5Cquad+%5Cbold%7B%5Cmu%7D%3D0%2C+%5Cquad+%5Cbold%7Bb%7D%3D0%5C%5C)

就完美地对号入座了！

        回忆一下，BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。**虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。**

        另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D%3D0). 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D) 。

**3.3.3 附录**

          WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

- WN是通过重写深度网络的权重来进行加速的，没有引入对minibatch的依赖，更适合于RNN网络。
- 引入更少的噪声。
- 不需要额外的空间进行存储minibatch的均值和方差，对时间的开销也小，所以速度会更快。

## 3.4 Instance Normalization

**3.4.1 动机：**

        作者指出在图像风格迁移任务中，生成式模型计算得到的 Feature Map 的各个 Channel 的均值与方差将影响到所生成图像的风格。故作者提出了 IN，在 Channel 层面对输入数据进行归一化，再使用目标图像的Channel 的均值与方差对结果进行 ‘去归一化’。一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。

**3.4.2 原理：**

<img src="https://pic2.zhimg.com/80/v2-e7f5074c1660dd1a9b4bcb029afcea95_720w.jpg" title="" alt="" data-align="center">

        前面提到，BN解决了模型ICS的问题，其实是将一个mini batch的分布重新映射了一遍，即基于batch的domain adaptation。但在Style Transfer任务中，模型更加关注每一张的特征（Style Transfer需要重构每张图的结构，而不是在一个batch内提取某些共有的feature）。因此，在Style Transfer中，每一张图片都是一个domain。针对每个feature map，我们在H,W维度将它其归一化，这便是IN。

<img src="https://pic3.zhimg.com/80/v2-6a5eb99a8ff1b55c98297f903d0bcfe6_720w.jpg" title="" alt="" data-align="center">

        由于很多Style Transfer任务仅仅是改变像素位置的颜色，因此，我们将通道维度也排除在归一化的范围外，这也是IN和LN的主要区别。其次，IN一般不会使用再放缩参数(affine = False)。和BN一样，IN需要计算running_mean, running_var。

        目前的结论是：在图片分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。

**3.4.3 附录**

​（1）实现步骤

1. IN的计算就是把每个HW单独拿出来标准化处理+缩放和平移

（2）IN的使用位置:

保持了每个图像的独立性,最初用于图像的风格化迁移

（3）IN的优点:

- 不受通道和batchsize 的影响

- 在图片视频分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。

## 3.5 Group Normalization

**3.5.1 动机：**

GN的动机也是为了解决BN受限于batch size的问题。

**3.5.2 原理：**

<img src="https://pic2.zhimg.com/80/v2-aa7deb543ede86de9cf954ae41315935_720w.jpg" title="" alt="" data-align="center">

        回到BN，实验表明，BN在batchsize较大的时候效果也得到提升（batchsize通常大于32），但是在大多数情况下，由于计算资源有限，我们的训练方案无法加载大的batchsize。这时，Group Normalization(GN)成为了一个解决方案。

GN将特征在通道维度（C）划分成若干个Group，然后对每个Group单独做normalization操作。对一个大小为 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+C+%5Ctimes+H+%5Ctimes+W) 的特征图，将它在 ![[公式]](https://www.zhihu.com/equation?tex=C) 维度分成 ![[公式]](https://www.zhihu.com/equation?tex=G) 组（ ![[公式]](https://www.zhihu.com/equation?tex=G) 为超参数）。显然，IN和LN分别是是 ![[公式]](https://www.zhihu.com/equation?tex=G%3DC) 的和 ![[公式]](https://www.zhihu.com/equation?tex=G%3D1) 特殊情况。在弥补了BN依赖大batchsize的缺点后，GN也集合了IN和LN的优势，一方面它可以学到channel维度不同位置的分布信息（IN），同时，他也能关注到channel之间的分布的关系（LN）。

**3.5.3 附录**

（1）实现步骤

1. 把C分成G组,每一组有C/G个通道,各组通道分别标准化+缩放和平移

（2）GN的优缺点:

1. 不受batch size 的影响,batch size很小的时候,GN的效果会比BN好.
2. GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C
3. 但对于大batch size，GN仍然难以匹敌BN。

## 3.6 **Switchable Normalization**

**3.6.1 原理**

将 BN、LN、IN 结合，赋予不同的权重，让网络自己去学习这些权重

![](https://pic4.zhimg.com/v2-4a783611e03d1272fb28fa3a5892951b_r.jpg)

![](https://pic4.zhimg.com/v2-09dcabe7a5a99cd2ddea11b76bad65af_r.jpg)

![](https://pic3.zhimg.com/v2-bcdd6a3a7e077a45cd42d21dd18e0686_r.jpg)

**3.6.2 附录**

缺点:训练复杂

## 7.1 Filter Response Normalization

**7.1.1 原理**

        谷歌的提出的FRN层包括归一化层FRN（Filter Response Normalization）和激活层TLU（Thresholded Linear Unit）,如图所示：

![](https://pic1.zhimg.com/v2-47c07c7138c696f2e5debd8cfed22614_r.jpg)

![](https://pic1.zhimg.com/v2-9dfbfcfe160491a5d538fa61315c4c5c_r.jpg)

![](https://pic2.zhimg.com/v2-fbdb63fd53aea730b404acae942b4bd9_r.jpg)

        标准化之后同样需要进行缩放和平移变换.FRN缺少去均值的操作，这可能使得标准化的结果任意地偏移0，如果FRN之后是ReLU激活层，可能产生很多0值，这对于模型训练和性能是不利的。为了解决这个问题，FRN之后采用的阈值化的ReLU，即TLU：

![](https://pic4.zhimg.com/v2-73f93800445177da6e5190495e738e93_r.jpg)

        ​这里的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 是一个可学习的参数。论文中发现FRN之后采用TLU对于提升性能是至关重要的.

**7.1.2 附录**

FRN的优缺点:

1. FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。
2. ​可以看到FRN是不受batch size的影响，而且效果是超越BN的。论文中还有更多的对比试验证明FRN的优越性。
3. BN目前依然是最常用的标准化方法，GN虽然不会受batch size的影响，但是目前还没大范围采用，不知道FRN的提出会不会替代BN，这需要时间的检验。

# 3.8 Cosine Normalization

**3.8.1 动机：**

        对输入数据$\mathbf{x}$的变换已经做过了，横着来是 LN，纵着来是 BN。对模型参数W $\mathbf{W}$的变换也已经做过了，就是 WN。那还有什么可做的呢？大佬们盯上了神经网络运算过程中的**内积**计算，即$f(x)=\mathbf{w} \cdot \mathbf{x}$中的“$\cdot$”。

        在经过深层网络结构之后，可能数据分布的方差已经很大，从而内积计算的结果也会很大，进而导致梯度爆炸等一系列问题。IN的目的是将这个结果值标准化到一个特定的区间

**3.8.2 原理：**

        我们再来看看神经元的经典变换 ![[公式]](https://www.zhihu.com/equation?tex=f_%5Cbold%7Bw%7D%28%5Cbold%7Bx%7D%29%3D%5Cbold%7Bw%7D%5Ccdot%5Cbold%7Bx%7D).对输入数据 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的变换已经做过了，横着来是 LN，纵着来是 BN。对模型参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 的变换也已经做过了，就是 WN。好像没啥可做的了。然而天才的研究员们盯上了中间的那个点，对，就是![[公式]](https://www.zhihu.com/equation?tex=%5Ccdot)

他们说，我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 特征数据向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的点积。向量点积是无界（unbounded）的啊！

        那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围，多么的美好！仿佛看到了新的世界！于是，Cosine Normalization 就出世了。他们不处理权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) ，也不处理特征数据向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) ，就改了一下线性变换的函数：

![[公式]](https://www.zhihu.com/equation?tex=f_%5Cbold%7Bw%7D%28%5Cbold%7Bx%7D%29%3Dcos+%5Ctheta+%3D+%5Cfrac%7B%5Cbold%7Bw%7D%5Ccdot%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cw%7C%7C%7D%5Ccdot%5Cbold%7B%7C%7Cx%7C%7C%7D%7D%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的夹角。然后就没有然后了，所有的数据就都是 [-1, 1] 区间范围之内的了！

        不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 的内积，而分母则可以看做用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D) 二者的模之积进行规范化。对比一下 WN 的公式：

![[公式]](https://www.zhihu.com/equation?tex=+f_%5Cbold%7Bw%7D%28WN%28%5Cbold%7Bx%7D%29%29%3Df_%5Cbold%7Bv%7D%28g%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%29%5C%5C)

一定程度上可以理解为，WN 用 权重的模 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cv%7C%7C%7D) 对输入向量进行 scale，而 CN 在此基础上用输入向量的模 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%7C%7Cx%7C%7C%7D) 对输入向量进行了进一步的 scale.

        CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。

# 3.9 总结

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201215113941183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0F1Z3VzdE1l,size_16,color_FFFFFF,t_70)

在深度学习时代，由于神经网络的不断发展，其在各项任务中均取得了不凡的效果。不过，要想从训练一个好神经网络也并非易事，比如，会遇到训练速率慢以及效果不如预期等问题。为此，本文从标准化方法（Normalization Methods）的角度来讲述其是如何影响和促进神经网络的发展和应用的。  

![](https://pic3.zhimg.com/v2-9352b6eb64ca75dcff8f61004ebe4212_r.jpg)

本文简要介绍以下几个常见的Normalization Methods及其变种：  

- **Batch Normalization-BN**
- **Weight Normalization-WN**
- **Instance Normalization-IN**
- **Layer Normalization-LN**
- **Group Normalization-GN**
- **Positional Normalization-PONO**
- **Conditional Batch Normalization-CBN**
- **Conditional Instance Normalization-CIN**
- **Adaptive Instance Normalization-AdaIN**
- **Spatially-adaptive normalization-SPADE**

**1 Batch Normalization-BN**  
Batch Normalization主要用于加速网络的训练过程，其最早于2015年由Sergey Ioffe和Christian Szegedy在[1]中提出。当时人们发现训练神经网络是一件比较麻烦的事情，因为在训练过程中，每层（layer）输入的分布（distribution）都会因为前面layers中参数的变化而变化。这样使得训练过程需要更小的学习率（learning rate）和精心设计的参数初始化（parameter initialization）。作者将该现象定义为internal covariate shift（ICS），为了解决该问题，作者尝试以mini-batch为单位对每个layer的输入进行normalizing，即所谓的Batch Normalization。  
对于一个size为 ![[公式]](https://www.zhihu.com/equation?tex=m) 的mini-batch ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BB%7D) ，BN所进行的操作是独立于每个特征channel的，对于输入数据 ![[公式]](https://www.zhihu.com/equation?tex=x) ，首先进行normalizing，得到zero mean和unit variance的![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D)，然后经过Batch Normalizing Transform得到 ![[公式]](https://www.zhihu.com/equation?tex=y) ，其中BN中要学习的参数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 。BN的过程可简单表述为 ![[公式]](https://www.zhihu.com/equation?tex=y%3DBN_%7B%5Cgamma%2C%5Cbeta%7D%28x%29) 。BN Transform算法的详细流程如下所示：  

![](https://pic4.zhimg.com/v2-8099767197b0586f388fa87f3a29769b_r.jpg)

算法1.应用于mini-batch上的BN Transform

需要注意的是BN在training的时候使用的是mini-batch的统计量进行normalizing，但是在inference阶段则是以popular statistics作为代替，这样会在这两个过程之间造成一定程度上的差异。以下是BN在network中training和inference的过程：  

![](https://pic1.zhimg.com/v2-66184c8a1b26ad215e14bed424321408_r.jpg)

算法2. BN在网络中的training和inference

![](https://pic2.zhimg.com/v2-3da7309e86c249f1721805c7e71879e1_r.jpg)

BN的操作图示

在实际使用过程中，主要将BN插入在network中的convolutional layer和nonlinearity function之间。例如传统卷积操作为：  
![[公式]](https://www.zhihu.com/equation?tex=z+%3D+g%28Wu+%2B+b%29%5C%5C) 其中 ![[公式]](https://www.zhihu.com/equation?tex=u) 为input， ![[公式]](https://www.zhihu.com/equation?tex=W) 为卷积核参数， ![[公式]](https://www.zhihu.com/equation?tex=b) 为bias， ![[公式]](https://www.zhihu.com/equation?tex=z) 为output。而BN操作则会应用到 ![[公式]](https://www.zhihu.com/equation?tex=Wu%2Bb) 上，即 ![[公式]](https://www.zhihu.com/equation?tex=x%3DBN%28Wu%2Bb%29%5Crightarrow%7BBN%28Wu%29%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=b) 的影响因为求mean而被消除。因此，在network中，BN操作可表示为：  
![[公式]](https://www.zhihu.com/equation?tex=z+%3D+g%28BN%28Wu%29%29%5C%5C)  

![](https://pic4.zhimg.com/v2-b105a71b4213af7171cc60761677ed07_r.jpg)

CNN基本单元. (a)convolution layer和non-linearity layer (b)convolution layer, BN layer和non-linearity layer.

**2 Weight Normalization-WN**  
WN由Salimans等人在[2]中提出，WN是对网络权值W进行normalization，故被称为Weight Normalization。BN在使用中会存在一些不足，例如BN的计算依赖于mini-Batch，对于RNN（LSTM）等网络是不能直接使用的，因为RNN是变长的，而且是基于time step进行计算；另外BN是通过mini-Batch计算mean和variance，而非基于所有的training data，这就相当于在梯度计算中引入了噪声，所以BN不适用于对噪声敏感的强化学习、生成模型等任务中。WN则不存在以上的问题，因为WN通过对网络参数 ![[公式]](https://www.zhihu.com/equation?tex=W) 进行重写（Reparameterization）的方式来加速深度学习网络参数收敛，没有引入mini-Batch的依赖，故可适用于RNN网络；另外，WN对通过标量 ![[公式]](https://www.zhihu.com/equation?tex=g) 和向量 ![[公式]](https://www.zhihu.com/equation?tex=v) 对权重 ![[公式]](https://www.zhihu.com/equation?tex=W) 进行重写，重写向量 ![[公式]](https://www.zhihu.com/equation?tex=v) 是固定的，因此，基于WN的Normalization可以认为比BN操作引入的噪声更少。  
一个神经网络中的激活单元的计算方式如下：![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Cphi%28%5Cmathbf%7Bw%7D+%5Ccdot+%5Cmathbf%7Bx%7D%2Bb%29%5C%5C)其中， ![[公式]](https://www.zhihu.com/equation?tex=W) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=k) 维的权重向量， ![[公式]](https://www.zhihu.com/equation?tex=b) 是一个偏置项标量， ![[公式]](https://www.zhihu.com/equation?tex=x) 是 ![[公式]](https://www.zhihu.com/equation?tex=k) 维输入特征，![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28%5Ccdot%29) 代表非线性激活函数， ![[公式]](https://www.zhihu.com/equation?tex=y) 是输出激活单元的输出标量。  
WN提出使用参数向量 ![[公式]](https://www.zhihu.com/equation?tex=v) 和标量 ![[公式]](https://www.zhihu.com/equation?tex=g) 来reparameterize权重向量 ![[公式]](https://www.zhihu.com/equation?tex=W) ，WN的计算方式如下：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D%3D%5Cfrac%7Bg%7D%7B%5C%7C%5Cmathbf%7Bv%7D%5C%7C%7D+%5Cmathbf%7Bv%7D%5C%5C) 其中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=k) 维向量， ![[公式]](https://www.zhihu.com/equation?tex=g) 是一个张量， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7C%5Cmathbf%7Bv%7D%5C%7C) 表示 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D) 的欧几里得范数。因为 ![[公式]](https://www.zhihu.com/equation?tex=g%3D%5C%7C%5Cmathbf%7Bw%7D%5C%7C)，这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。  
**3 Instance Normalization-IN**  
IN主要用于图像的风格化（Image Stylization）任务中，其最早于2017年由Dmitry Ulyanov等人在[3]中提出。Image Stylization是指生成器（generator）根据一副内容图像（content image）和风格图像（style image）生成新的图像，而该图像在内容上与content image保持一致，但在风格上与style image保持一致。  

![](https://pic1.zhimg.com/v2-0d8861545b57199b05bd609771dbea40_r.jpg)

早先Gatys 等人对此提出了基于迭代式优化的方法（Iterative Optimization Method ），该方法生成的效果不错，但缺点是生成的过程需要大量的时间，无法做到实时（real-time）。为此，Dmitry Ulyanov等人对generator的网络结构进行了改进，提出了Instance Normalization （IN），用以解决image stylization过程中速度过慢的问题。  
对于输入size为 ![[公式]](https://www.zhihu.com/equation?tex=N) 的一个batch张量![[公式]](https://www.zhihu.com/equation?tex=+x+%5Cin+%5Cmathbb%7BR%7D%5E%7BN+%5Ctimes+C+%5Ctimes+W+%5Ctimes+H%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bnijk%7D) 定义为其第 ![[公式]](https://www.zhihu.com/equation?tex=nijk) 个元素，其中 ![[公式]](https://www.zhihu.com/equation?tex=k) 和 ![[公式]](https://www.zhihu.com/equation?tex=j) 为空间维度， ![[公式]](https://www.zhihu.com/equation?tex=i) 指特征通道， ![[公式]](https://www.zhihu.com/equation?tex=n) 是图像在batch中的索引，则IN定义为：  

![](https://pic4.zhimg.com/v2-eac90e29cc83e61600c0f5038b530947_r.jpg)

Instance Normalization （IN）操作

看其形式，会发现比较眼熟，没错，就是和BN十分相似。如果按照以上形式描述BN操作，则如下所示：  

![](https://pic3.zhimg.com/v2-795e3bd185044659cf6fe4e079356372_r.jpg)

Batch Normalization （BN）操作

可以发现，IN和BN的主要区别在于两点：首先，IN对每个单独的content image进行normalize，其操作独立于每个channel和sample，而非BN操作中依赖于batch size的大小，因此在实现中，只要对BN进行简单的替换即可实现IN操作；其次，在inference阶段，IN的也会使用，而BN则会使用popular statistics进行代替。  

![](https://pic2.zhimg.com/v2-e986b535dd74f5dd45a572610b8a9939_r.jpg)

IN的操作图示

**4 Layer Normalization-LN**  
Layer Normalization最早由Hinton等人于2016年在[4]提出，LN主要是为了解决BN的计算必须依赖mini-batch的size大小，导致其不能在诸如RNN等循环神经网络中使用（因为不同的time-step对应不同的statistics）。  
对于一个layer中所有hidden units计算LN的方式如下：  

![](https://pic3.zhimg.com/v2-0e4da0147cd162647da13b761e96b4ba_r.jpg)

Layer Normalization（LN）操作

这里H表示layer中hidden units的数量，为了阐明与BN的关系，再将BN的操作以该形式进行表达：  

![](https://pic2.zhimg.com/v2-bf5ece905c440f1487d8223c93beca11_r.jpg)

Batch Normalization （BN）操作

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7Ba%7D_%7Bi%7D%5E%7Bl%7D) 是第 ![[公式]](https://www.zhihu.com/equation?tex=l%5E%7Bth%7D) 层layer中第 ![[公式]](https://www.zhihu.com/equation?tex=i%5E%7Bth%7D) 个hidden unit的normalized summed inputs， ![[公式]](https://www.zhihu.com/equation?tex=g_%7Bi%7D) 是gain parameter用来缩放非线性激活函数之前的normalized activation。  
从以上两式中可以看出二者的不同，对于LN，同一layer下的所有hidden units同享相同的normalization项—— ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) ，但是不同的训练实例有不同的normalization项；LN不同于BN，它不会引入任何受限于mini-Batch的size大小的限制。  

![](https://pic2.zhimg.com/v2-4de7cf0c3e2ba23f453c1f2ca3e54a91_r.jpg)

(a)BN中计算normalization需要考虑mini-batch中所有的samples；(b)LN中的normalization则不会在sample之间产生影响

![](https://pic4.zhimg.com/v2-3bfc63dd5189c1c76fee4ad3353fa363_r.jpg)

LN 的操作图示

**5 Group Normalization-GN**  
Group Normalization由Kaiming He等人于2018年在[5]提出，GN同样是为了解决BN依赖于batch size的不足。BN在normalizing的时候是沿着batch维度的，因此充分大的batch size才会比较有利于BN。不过在某些任务中，例如object detection，segmentation或者video classification等，batch size不能太大，否则就会导致batch statistics计算上的不精确，从而影响最终的性能，如下图所示：  

![](https://pic2.zhimg.com/80/v2-42e1375a64175e537dcb4b47ad5dc991_720w.jpg)

batch size大小对ImageNet分类任务的影响。随着batch size的减小，BN的error逐渐增长。

GN操作的计算方式如下所示： ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D_%7Bi%7D%3D%5Cfrac%7B1%7D%7B%5Csigma_%7Bi%7D%7D%5Cleft%28x_%7Bi%7D-%5Cmu_%7Bi%7D%5Cright%29%5C%5C)  
这里 ![[公式]](https://www.zhihu.com/equation?tex=x) 是由layer得到的特征， ![[公式]](https://www.zhihu.com/equation?tex=i) 为其索引。 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 分别为mean和standard deviation (std)，计算方式如下：  
![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bi%7D%3D%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bk+%5Cin+%5Cmathcal%7BS%7D_%7Bi%7D%7D+x_%7Bk%7D%2C+%5Cquad+%5Csigma_%7Bi%7D%3D%5Csqrt%7B%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bk+%5Cin+%5Cmathcal%7BS%7D_%7Bi%7D%7D%5Cleft%28x_%7Bk%7D-%5Cmu_%7Bi%7D%5Cright%29%5E%7B2%7D%2B%5Cepsilon%7D%2C%5C%5C)  
这里 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 是一个很小的常量， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D_%7Bi%7D) 在这里泛指用来计算mean和std的pixels所构成的set， ![[公式]](https://www.zhihu.com/equation?tex=m) 为该set的大小。实际上，对于BN，IN和LN这些normalization方法来说，其主要的不同之处就在于set ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D_%7Bi%7D)的定义方式不同，对于GN而言，其![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D_%7Bi%7D)定义如下：  
![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D_%7Bi%7D%3D%5Cleft%5C%7Bk+%7C+k_%7BN%7D%3Di_%7BN%7D%2C%5Cleft%5Clfloor%5Cfrac%7Bk_%7BC%7D%7D%7BC+%2F+G%7D%5Cright%5Crfloor%3D%5Cleft%5Clfloor%5Cfrac%7Bi_%7BC%7D%7D%7BC+%2F+G%7D%5Cright%5Crfloor%5Cright%5C%7D%5C%5C) 这里 ![[公式]](https://www.zhihu.com/equation?tex=G) 是指group的数量，这是一个需要预定义的超参。 ![[公式]](https://www.zhihu.com/equation?tex=C%2FG) 是指每个group中channel的数量。 ![[公式]](https://www.zhihu.com/equation?tex=%5Clfloor%5Ccdot%5Crfloor) 是指floor操作。 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5Clfloor%5Cfrac%7Bk_%7BC%7D%7D%7BC+%2F+G%7D%5Cright%5Crfloor%3D%5Cleft%5Clfloor%5Cfrac%7Bi_%7BC%7D%7D%7BC+%2F+G%7D%5Cright%5Crfloor) 是指索引 ![[公式]](https://www.zhihu.com/equation?tex=k) 和 ![[公式]](https://www.zhihu.com/equation?tex=i) 存在于channel的相同group中，假设channel的每个group是沿着 ![[公式]](https://www.zhihu.com/equation?tex=C) 轴以顺序的方式进行存储的。GN是沿着 ![[公式]](https://www.zhihu.com/equation?tex=%28H%2CW%29) 轴和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BC%7D%7BG%7D) channel的group轴来计算 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 的。需要指出的是，同一个group中的pixels被normalized为相同的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 。  
![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%3D%5Cgamma+%5Chat%7Bx%7D_%7Bi%7D%2B%5Cbeta%5C%5C) ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 是trainable的scale和shift，用来学习per-channel的线性变换以便对表达能力的损失进行补偿。  

![](https://pic3.zhimg.com/v2-ac5abcdf8e28d1e3020c802fbeb5288a_r.jpg)

GN的操作图示

**6 Positional Normalization-PONO**  
PONO由Li等人于2019年在[6]中提出。以上几个normalization方法的一个共同特点是它们都会在某种方式上进行across spatial dimensions的normalize，也就是说在空间上是以一种全局的方式进行处理。而PONO则从另外一个角度出发，在每个特征的spatial position上独立的进行across channels的normalize，因此该方式提取到的mean和standard deviation能够捕获输入图像的coarse structural information。效果如下图所示：  

![](https://pic3.zhimg.com/v2-9edf9ee5e4a4d9a0c0b3718d9f654086_r.jpg)

PONO在VGG-19的不同layer中提取的不同µ和σ能捕获输入图像的结构信息

PONO的计算方式如下： ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bb%2C+h%2C+w%7D%3D%5Cfrac%7B1%7D%7BC%7D+%5Csum_%7Bc%3D1%7D%5E%7BC%7D+X_%7Bb%2C+c%2C+h%2C+w%7D%2C%5C%5C)  
![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7Bb%2C+h%2C+w%7D%3D%5Csqrt%7B%5Cfrac%7B1%7D%7BC%7D+%5Csum_%7Bc%3D1%7D%5E%7BC%7D%5Cleft%28X_%7Bb%2C+c%2C+h%2C+w%7D%5E%7B2%7D-%5Cmu_%7Bb%2C+h%2C+w%7D%5Cright%29%2B%5Cepsilon%7D%2C%5C%5C)  
其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) 是一个小值稳定常量（ ![[公式]](https://www.zhihu.com/equation?tex=e.g.) ， ![[公式]](https://www.zhihu.com/equation?tex=10%5E%7B-5%7D) ）。  
PONO与BN、IN、GN和LN的运算方式的区别如下图所示：  

![](https://pic3.zhimg.com/v2-9468437549f5b08a8ec4613cd0167a0a_r.jpg)

图中B为batch轴，C为channel轴，(H,W)为spatial轴。着色的部分（绿和蓝）使用相同的mean和std进行normalize。PONO独立的处理每个position，并跨channels计算两个统计数据。

论文中作者还认为，PONO比较适合用于生成模型（generative model），因此提出了Moment Shortcut（MS）的概念，用以实现具有结构化信息的图像生成任务（image generation task）。以autoencoder为例，首先在encoder中提取具有positional moment information的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) ，然后送入到decoder的对应layer中。其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 是以相加的方式， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 是以相乘的方式。具体来讲，MS的计算方式如下：  
![[公式]](https://www.zhihu.com/equation?tex=MS%28x%29+%3D+%CE%B3F%28x%29+%2B+%CE%B2%2C%5C%5C)这里 ![[公式]](https://www.zhihu.com/equation?tex=F) 由中间层进行建模， ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%2C%5Cbeta) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%2C%5Csigma+) 分别由输入 ![[公式]](https://www.zhihu.com/equation?tex=x) 得到。MS使得decoder中layers产生的激活信息与encoder中对应layers产生的统计信息相似。MS可以单独或与PONO结合使用，PONO-MS（PONO结合MS）的特点是可以从输入图像中提取到必要的structural information，因此可以利用其实现将一张image的structure从其encoder迁移到另一张图像的decoder中。MS和DMS的示意图如下所示：  

![](https://pic1.zhimg.com/v2-b6af89b74db50966f956676def46c8a0_r.jpg)

左侧：PONO-MS直接使用µ和σ作为 β和 γ。右侧：PONO-DMS使用一个浅层网络基于µ和σ来预测 β和 γ，故为Dynamic Moment Shortcut（DMS）。

**7 Conditional Batch Normalization-CBN**  
CBN的思想最早由Harm等于2017年在[7]中提出。该论文主要介绍了一种Visual Question Answering（VQA）系统，大致来讲，作者为了进一步的将句子语义与图像内容进行对应，提出了将句子的特征与图像的不同层次的特征进行结合。具体做法是：首先获取图像feature map中的BN参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ；然后利用句子的LSTM特征作为condition来强调或者抑制feature map中的某些channel；最后则是使用MLP，以LSTM特征为输入，以增量 ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%5Cbeta) 为输出，来修改最初图像feature map中的![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta)。这样就使得在图像的底层和高层feature中融合了句子的信息，并最终取得了比较明显的提升。  
传统的 Batch Normalization (BN) 公式为：  
![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Cfrac%7Bx-%5Cmathbb%7BE%7D%5Bx%5D%7D%7B%5Csqrt%7B%5Coperatorname%7BVar%7D%5Bx%5D%2B%5Cepsilon%7D%7D+%5Ccdot+%5Cgamma%2B%5Cbeta+%5C%5C)  
其中，参数![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta)需要通过反向传播来学习。而在CBN中，这些参数则是通过把feature输入到MLP中前向传播得到的网络输出，由于![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cgamma%7D)和 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbeta%7D) 依赖于输入的 feature 这个 condition，因此这个改进版的 BN 叫做 Conditional Batch Normalization(CBN)。  
![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Cfrac%7Bx-%5Cmathbb%7BE%7D%5Bx%5D%7D%7B%5Csqrt%7B%5Coperatorname%7BVar%7D%5Bx%5D%2B%5Cepsilon%7D%7D+%5Ccdot+%5Chat%7B%5Cgamma%7D%2B%5Chat%7B%5Cbeta%7D+%5C%5C)  
其中，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cgamma%7D)和 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cbeta%7D)分别作为更新后的参数以代替原来BN中的默认参数：![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+%5Chat%7B%5Cgamma%7D%3D%5Cgamma%2B%5CDelta+%5Cgamma+%5C%5C+%5Chat%7B%5Cbeta%7D%3D%5Cbeta%2B%5CDelta+%5Cbeta+%5Cend%7Barray%7D%5Cright.%5C%5C)  
因为该论文在设计框架时，使用的是预训练的ResNet作为图像特征提取器，因此在修改BN参数时，只是进行小范围的更新而没有直接替换原始参数。而在后续的论文《[cGANs with Projection Discriminator](https://arxiv.org/pdf/1802.05637.pdf)[![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAUhSURBVFhHtZdZiBxVFIZ7eq2u6VnijDOZsWcLyGDAJUgEFxTR4IsLOmrENRF8cceAGygoiBBFQY1roiQiIUFRcHkYNFEEHUENKCERXGIIbomGyYPOYvfx+2/X7VSPzmIsC3666tatc/77n+XeTs12VUd6AxsID9pgs9nAkeHe1ux2TBVrFue+RsAGsD6G1y4JM3Ztc9auAhcUMw4Xg2t4vg7o/YU8C5dzrzHhjpacVfub3TzsjJVSqTZ+57w2Ak22bFPKVgQZu6GUtVMLaTcmnM3YyjBrJ+UPjw1n07ZCBHDen2mqj6dBpS+0VTUC1rQAEuuAm7wo3WQTfDzaVbBdvUV7pD3vVipJt3YWbO8xRXvr6IJlmfvZ4oDn0N7pCpxS3kYnNuIEhPlINBAYL4fWze9qDOyFhGTdw6/ed7NSkZHs23G8dlHejRVQztvwBKIQ1DEXiTqBdj7+mVV+0aPVFd0qRlBgH/d+zk/c39qStVdR5ACOdkNOueHfi4Axfv0MAhHGQCdouBoUOIQCa0ikoWwtrjL0Y0TgIhxJgeNzaRvrDuyh9pyFrF4h8TZEYLxcdKo9f1TenothQ0fBtnTktjCv4WogcAACy/OHV7QSBSbJ6k+J+Z5o9Rr/nOd7WnP1eR4i8AvzKGNHtgFDJSqk+D7zGq4XgftYGdzPynOxmAqKs1fEI5gxx0Nq7BeB/tAmCUUcIjFRDkaZ13AtA6tjWLU0k74bySovINvT4DGS7XHwFJCUkvYJ7tcB3cexGZn/iJwJSsj5CPztqp4YdlYHwoo3Mi/iHTO6H6VK3qBkHZlIDY0viMBET+5Y+kHlzxj72aC+odKVEzmQMsJWnKuhLaN5qaIq0ftECei9HJ9Pdagy7m/L2ZOEoZVEbAO9UYf01ZMogSkgg3KoXLmTijgOEkpYOVdCY8ohT8LuoywVnsQIyPmbyMx020jiqcSkhsLxA5KreamVn0wIShD4hL6RGAFl9iFQjiT+RqvDuVRxsQaao/1DlXNmIWPfJ6mAVv8uGxZTHTZ15J3x+PuXUUXSG03Mk0ssB2ToYyRlqoP2kB10Rk9C7+8iJ5SMslGNfZcIgWlQYWXaqJju0EU4tlH7IiFHb0f58W0UHn2XGAFBsVaTuTK267Hluq4pR8JSquJq3seVSYyAoJXJqJJNfYBPHXR6GuedNi896+zgSSVKQFA4nGF+n6Ef+E1rkJDsRP73cD7I2K/qlklVwT9BZSci6gNrqX8lIKbsdXLhJYjdTFu2of+RgMqsGoVEjvZD5EacYs7lxaOQ+hpFqIjkCGjVfo7yQQeR3T1F+0hdLyKiRoRJd3jRISYRAlqt3imu+lVsN9F4dJA9hyM8JuwySnRKivDuDI74OuR8hwKQ/m8EXEfD6Aes8iwMD5P9O1i1zpEncK+tdyBKRP1JsSUle5izo543Uy3VwSMgIKceaq33se0qyZ4luU5hs1En1ElIfUCnZXXJZlbckk7ZNCpIGcy6+f+KgFqoq/V6cpXcnxam2C0cTqcZ28Xq9XwaajwIMd2fRxiEc8GlhELENL6TuZy0FkaA1VZ+I8brYa1udzoObkPSJcgrZyLkTzk6G/KZXUEDeiAiMRP6/6iknFpQGS7ODavBSH5l9pcw138A/QPqQO6vXH+vtVcXFkh8iOw3oYr2fa10DVmvHFlOiG5nXNu3cuf3crAtcjP7Ve0LeibLwUHVtet00Ur1p0XQs09KDxciqRKbHw+f7xETfcErNS+p1F9b1z/MoMKQ6AAAAABJRU5ErkJggg==)](https://arxiv.org/pdf/1802.05637.pdf)》和《[Self-Attention Generative Adversarial Networks](http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf)》中继承了该condition的思想，其设计的conditional GANs没有用到预训练的网络，而是根据图像的类别信息进行训练，直接预测出 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 作为新的条件参数。  
**8 Conditional Instance Normalization-CIN**  
CIN 最早由Dumoulin等于[8]中提出，主要用于实现图像的风格迁移。相比利用Instance Normalization实现单一的style transfer，CIN则可以灵活的learn multiple styles。以下将这两种方法进行对比说明。  
对于Instance Normalization，其计算方式如下：  
![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7BIN%7D%28x%29%3D%5Cgamma%5Cleft%28%5Cfrac%7Bx-%5Cmu%28x%29%7D%7B%5Csigma%28x%29%7D%5Cright%29%2B%5Cbeta%5C%5C) 对于Conditional Instance Normalization，其计算方式如下：  
![[公式]](https://www.zhihu.com/equation?tex=%5Coperatorname%7BCIN%7D%28x+%3B+s%29%3D%5Cgamma%5E%7Bs%7D%5Cleft%28%5Cfrac%7Bx-%5Cmu%28x%29%7D%7B%5Csigma%28x%29%7D%5Cright%29%2B%5Cbeta%5E%7Bs%7D%5C%5C) 这两个计算公式看似相似，实则不然。CIN中的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 不再是IN中自行去学习，而是在训练中学习不同的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Bs%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta%5E%7Bs%7D) pairs。这样做的好处是可以使用网络的一次feed forward pass就可以实现对一张content image的N种风格化，而对于single-style network则需要N个 feed forward passes来实现N种style transfers。  

![](https://pic2.zhimg.com/v2-44cd51581c575a83ac1c71b11c87cb81_r.jpg)

CIN的计算过程

CIN的计算过程。其输入激活 ![[公式]](https://www.zhihu.com/equation?tex=x) 首先在两个空间维度上都进行了归一化，然后使用style-dependent的参数矢量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Bs%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta%5E%7Bs%7D) 进行缩放和变换，其中 ![[公式]](https://www.zhihu.com/equation?tex=s) 为style的label。  
**9 Adaptive Instance Normalization-AdaIN**  
AdaIN由Huang等人在[9]中提出，这是一种能够进行实时的任意风格迁移的方法。有了IN和CIN的对style transfer的相关知识，理解AdaIN就很容易了。IN告诉我们，图像的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 代表着其统计信息mean和variance，对应着它的artistic style。而CIN则进一步指出，使用不同的![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 即可生成出风格不同的图像，它是使用（ ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Bs%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta%5E%7Bs%7D) ）对进行学习的；不过该方法只能迁移有限种的风格，想迁移新的的风格则需要训练新的模型。AdaIN则很好的解决了该问题，它利用一张输入图像y作为style image，计算其对应的风格信息 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%28y%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28y%29) ；然后作用到另外一张图像x作为content image上，实现将y的style迁移到x的content上。AdaIN的计算方式如下所示：  
![[公式]](https://www.zhihu.com/equation?tex=%5Coperatorname%7BAdaIN%7D%28x%2C+y%29%3D%5Csigma%28y%29%5Cleft%28%5Cfrac%7Bx-%5Cmu%28x%29%7D%7B%5Csigma%28x%29%7D%5Cright%29%2B%5Cmu%28y%29%5C%5C) 对于AdaIN而言，只需要两张图象，分别为style image和content image就可以了，从而不再需要训练新的网络。  
**10 Spatially-Adaptive Normalization-SPADE**  
SPADE是由NVIDIA在[10]中提出的一种空间自适应归一化方法，主要用于图像合成，旨在解决先前图像合成任务中一些稀疏输入语义合成效果不好的问题。  
对于传统的normalization方法，例如BN和IN，实验证明它们存在一定的缺陷，即容易将语义信息给抹掉（wash away）。为了解决该问题，该论文提出的了一种空间自适应的归一化方法SPADE，这是一种条件归一化层，它使用输入的语义布局图通过以空间自适应方式学到的变换对激活输出进行调制，该方法可以有效的在神经网络中传播语义信息。  
论文中提出的SPADE block结构如下图所示：  

![](https://pic3.zhimg.com/80/v2-4651b38dd9c58472e7db01a3ab0f8896_720w.jpg)

SPADE的结构

输入的semantic mask首先投影到嵌入子空间中，再通过卷基层得到调制参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 。需要注意的是，与之前的条件归一化方法不一样，这里的![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 并非向量vectors，而是拥有空间维度的张量tensors。![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta)以element-wise的方式multiplied和added到normalized的activation上。  
SPADE的计算方式如下： ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma_%7Bc%2C+y%2C+x%7D%5E%7Bi%7D%28%5Cmathbf%7Bm%7D%29+%5Cfrac%7Bh_%7Bn%2C+c%2C+y%2C+x%7D%5E%7Bi%7D-%5Cmu_%7Bc%7D%5E%7Bi%7D%7D%7B%5Csigma_%7Bc%7D%5E%7Bi%7D%7D%2B%5Cbeta_%7Bc%2C+y%2C+x%7D%5E%7Bi%7D%28%5Cmathbf%7Bm%7D%29%5C%5C)  
其中， ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bn%2C+c%2C+y%2C+x%7D%5E%7Bi%7D) 表示归一化之前点的激活， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bc%7D%5E%7Bi%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7Bc%7D%5E%7Bi%7D) 分别指通道 ![[公式]](https://www.zhihu.com/equation?tex=c) 中activations的mean 和standard deviation： ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bc%7D%5E%7Bi%7D%3D%5Cfrac%7B1%7D%7BN+H%5E%7Bi%7D+W%5E%7Bi%7D%7D+%5Csum_%7Bn%2C+y%2C+x%7D+h_%7Bn%2C+c%2C+y%2C+x%7D%5E%7Bi%7D%5C%5C)  
![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7Bc%7D%5E%7Bi%7D%3D%5Csqrt%7B%5Cfrac%7B1%7D%7BN+H%5E%7Bi%7D+W%5E%7Bi%7D%7D+%5Csum_%7Bn%2C+y%2C+x%7D%5Cleft%28%5Cleft%28h_%7Bn%2C+c%2C+y%2C+x%7D%5E%7Bi%7D%5Cright%29%5E%7B2%7D-%5Cleft%28%5Cmu_%7Bc%7D%5E%7Bi%7D%5Cright%29%5E%7B2%7D%5Cright%29%7D%5C%5C)  
变量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma_%7Bc%2C+y%2C+x%7D%5E%7Bi%7D%28%5Cmathbf%7Bm%7D%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_%7Bc%2C+y%2C+x%7D%5E%7Bi%7D%28%5Cmathbf%7Bm%7D%29) 都是学到的归一化层的调制参数，它们取决于输入的分割蒙版，并随位置 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Cy%29) 而变化。  
实际上，SPADE是几种当前normalization方法的泛化形式。  

- 使用图像类别标签替换分割蒙版 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bm%7D)使得调制参数具有空间不变性（ ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma_%7Bc%2C+y_%7B1%7D%2C+x_%7B1%7D%7D%5E%7Bi%7D+%5Cequiv+%5Cgamma_%7Bc%2C+y_%7B2%7D%2C+x_%7B2%7D%7D%5E%7Bi%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_%7Bc%2C+y_%7B1%7D%2C+x_%7B1%7D%7D%5E%7Bi%7D+%5Cequiv+%5Cbeta_%7Bc%2C+y_%7B2%7D%2C+x_%7B2%7D%7D%5E%7Bi%7D) ），则会得到CBN的形式；
- 使用一张真实图像替换 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bm%7D) ，使得调制参数具有空间不变形，同时设置 ![[公式]](https://www.zhihu.com/equation?tex=N%3D1) ，则可以得到AdaIN的形式；

![](https://pic1.zhimg.com/v2-a5eb5a627f6b9c754ed0aedf6c711e00_r.jpg)

SPADE Generator结构。Generator中的每一层都加入了SPADE Residual Block结构，同时使用分割蒙版调制每层的激活输出。

Normalization Methods目前已经在深度学习中成为了一个重要的研究领域，它使得神经网络的优化变得更加容易。随着研究的不断的深入，将会有更多关于其新的发现的和新的应用。

前言：

归一化相关技术已经经过了几年的发展，目前针对不同的应用场合有相应的方法，在本文将这些方法做了一个总结，介绍了它们的思路，方法，应用场景。主要涉及到：LRN，BN，LN, IN, GN, FRN, WN, BRN, CBN, CmBN等。

本文又名“BN和它的后浪们”，是因为几乎在BN后出现的所有归一化方法都是针对BN的三个缺陷改进而来，在本文也介绍了BN的三个缺陷。相信读者会读完此文会对归一化方法有个较为全面的认识和理解。

**点个关注，每天更新两篇计算机视觉的文章。**

## LRN(2012)

局部响应归一化（Local Response Normalization, 即LRN）首次提出于AlexNet。自BN提出后，其基本被抛弃了，因此这里只介绍它的来源和主要思想。

LRN的创意来源于神经生物学的侧抑制，被激活的神经元会抑制相邻的神经元。用一句话来形容LRN：让响应值大的feature map变得更大，让响应值小的变得更小。

其主要思想在于让不同卷积核产生feature map之间的相关性更小，以实现不同通道上的feature map专注于不同的特征的作用，例如A特征在一通道上更显著，B特征在另一通道上更显著。

## Batch Normalization(2015)

论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

论文中关于BN提出的解释：训练深度神经网络非常复杂，因为在训练过程中，随着先前各层的参数发生变化，各层输入的分布也会发生变化，图层输入分布的变化带来了一个问题，因为图层需要不断适应新的分布，因此训练变得复杂，随着网络变得更深，网络参数的细微变化也会放大。

由于要求较低的学习率和仔细的参数初始化，这减慢了训练速度，并且众所周知，训练具有饱和非线性的模型非常困难。我们将此现象称为内部协变量偏移，并通过归一化层输入来解决该问题。

其它的解释：假设输入数据包含多个特征x1，x2，…xn。每个功能可能具有不同的值范围。例如，特征x1的值可能在1到5之间，而特征x2的值可能在1000到99999之间。

如下左图所示，由于两个数据不在同一范围，但它们是使用相同的学习率，导致梯度下降轨迹沿一维来回振荡，从而需要更多的步骤才能达到最小值。且此时学习率不容易设置，学习率过大则对于范围小的数据来说来回震荡，学习率过小则对范围大的数据来说基本没什么变化。

如下右图所示，当进行归一化后，特征都在同一个大小范围，则loss landscape像一个碗，学习率更容易设置，且梯度下降比较平稳。

![](https://pic1.zhimg.com/v2-4ec67b9086e7b2b9170c4eed2ec18460_r.jpg)

​ 实现算法：

![](https://pic1.zhimg.com/v2-bf2f4550c31b72d49c8be20d3420ef08_r.jpg)

​ 在一个batch中，在每一BN层中，对每个样本的同一通道，计算它们的均值和方差，再对数据进行归一化，归一化的值具有零均值和单位方差的特点，最后使用两个可学习参数gamma和beta对归一化的数据进行缩放和移位。

此外，在训练过程中还保存了每个mini-batch每一BN层的均值和方差，最后求所有mini-batch均值和方差的期望值，以此来作为推理过程中该BN层的均值和方差。

注：BN放在激活函数后比放在激活函数前效果更好。

实际效果：

1）与没有BN相比，可使用更大的学习率

2）防止过拟合，可去除Dropout和Local Response Normalization

3）由于dataloader打乱顺序，因此每个epoch中mini-batch都不一样，对不同mini-batch做归一化可以起到数据增强的效果。

4）明显加快收敛速度

5）避免梯度爆炸和梯度消失

注：BN存在一些问题，后续的大部分归一化论文，都是在围绕BN的这些缺陷来改进的。为了行文的方便，这些缺陷会在后面各篇论文中逐一提到。

## BN、LN、IN和GN的区别与联系

下图比较明显地表示出了它们之间的区别。（N表示N个样本，C表示通道，这里为了表达方便，把HxW的二维用H*W的一维表示。）

![](https://pic3.zhimg.com/v2-a88dce24a6c5cfcc75a121a03e6cafe6_r.jpg)

​ 后面这三个解决的主要问题是BN的效果依赖于batch size，当batch size比较小时，性能退化严重。可以看到，IN，LN和GN都与batch size无关。

它们之间的区别在于计算均值和方差的数据范围不同，LN计算单个样本在所有通道上的均值和方差，IN值计算单个样本在每个通道上的均值和方差，GN将每个样本的通道分成g组，计算每组的均值和方差。

它们之间的效果对比。（注：这个效果是只在同一场合下的对比，实际上它们各有自己的应用场景，且后三者在各自的应用场合上都明显超过了BN）

![](https://pic3.zhimg.com/v2-159a37dabddf4b684d88230be08caa6a_r.jpg)

​

## Instance Normalization(2016)

论文：Instance Normalization: The Missing Ingredient for Fast Stylization

在图像视频等识别任务上，BN的效果是要优于IN的。但在GAN，style transfer和domain adaptation这类生成任务上，IN的效果明显比BN更好。

从BN与IN的区别来分析产生这种现象的原因：BN对多个样本统计均值和方差，而这多个样本的domain很可能是不一样的，相当于模型把不同domain的数据分布进行了归一化。

## Layer Normalization (2016)

论文：Layer Normalization

BN的第一个缺陷是依赖Batch size，第二个缺陷是对于RNN这样的动态网络效果不明显，且当推理序列长度超过训练的所有序列长度时，容易出问题。为此，提出了Layer Normalization。

当我们以明显的方式将批归一化应用于RNN时，我们需要为序列中的每个时间步计算并存储单独的统计信息。如果测试序列比任何训练序列都长，这是有问题的。LN没有这样的问题，因为它的归一化项仅取决于当前时间步长对层的总输入。它还只有一组在所有时间步中共享的增益和偏置参数。(注：LN中的增益和偏置就相当于BN中的gamma 和beta)

LN的应用场合：RNN，transformer等。

## Group Normalization(2018)

论文：Group Normalization

如下图所示，当batch size减少时，BN退化明显，而Group Normalization始终一致，在batch size比较大的时候，略低于BN，但当batch size比较小的时候，明显优于BN。

![](https://pic1.zhimg.com/v2-d2f83be5833d8b1f25c51cb87b94e888_r.jpg)

但GN有两个缺陷，其中一个是在batchsize大时略低于BN，另一个是由于它是在通道上分组，因此它要求通道数是分组数g的倍数。

GN应用场景：在目标检测，语义分割等要求尽可能大的分辨率的任务上，由于内存限制，为了更大的分辨率只能取比较小的batch size，可以选择GN这种不依赖于batchsize的归一化方法。

GN实现算法

![](https://pic1.zhimg.com/v2-79ce71f28cf3712b71fce6ee6a3fe19c_r.jpg)

## Weights Normalization(2016)

论文：Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks

前面的方法都是基于feature map做归一化，这篇论文提出对Weights做归一化。

解释这个方法要费挺多笔墨，这里用一句话来解释其主要做法：将权重向量w分解为一个标量g和一个向量v，标量g表示权重向量w的长度，向量v表示权重向量的方向。

这种方式改善了优化问题的条件，并加速了随机梯度下降的收敛，不依赖于batch size的特点，适用于循环模型（如 LSTM）和噪声敏感应用（如深度强化学习或生成模型），而批量归一化不太适合这些应用。

Weight Normalization也有个明显的缺陷：WN不像BN有归一化特征尺度的作用，因此WN的初始化需要慎重，为此作者提出了对向量v和标量g的初始化方法。

## Batch Renormalization(2017)

论文：Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models

前面我们提到BN使用训练过程中每个mini-batch的均值和方差的期望作为推理过程中的均值和方差，这样做的前提是mini-batch与样本总体是独立同分布的。因此BN的第三个缺陷是当mini-batch中的样本非独立同分布时，性能比较差。

基于第一个缺陷batchsize太小时性能退化和第三个缺陷，作者提出了Batch Renormalization（简称BRN）。

BRN与BN的主要区别在于BN使用训练过程中每个mini-batch的均值和方差的期望来当作整个数据集的均值和方差，而训练过程中每个mini-batch都有自己的均值和方差，因此在推理阶段的均值和方差与训练时不同，而BRN提出在训练过程中就不断学习修正整个数据集的均值和方差，使其尽可能逼近整个数据集的均值和方差，并最终用于推理阶段。

BRN实现算法如下：

![](https://pic4.zhimg.com/v2-200cc2f0903f3902e2c32737b1d19d43_r.jpg)

注：这里r和d表示尺度缩放和平移，不参与反向传播。

当使用小batchsize或非独立同分布的mini-batch进行训练时，使用BRN训练的模型的性能明显优于BN。同时，BRN保留了BN的优势，例如对初始化的敏感性和训练效率

## Cross-GPU BN(2018)

论文：MegDet: A Large Mini-Batch Object Detector

在使用多卡分布式训练的情况下，输入数据被等分成多份，在各自的卡上完成前向和回传，参数更新，BN是针对单卡上的样本做的归一化，因此实际的归一化的样本数并不是batchsize。例如batchsize=32，用四张卡训练，实际上只在32/4=8个样本上做归一化。

Cross-GPU Batch Normalization的思想就是在多张卡上做归一化。

具体实现算法如下：

![](https://pic1.zhimg.com/v2-612be9705ca6e9274350039ff6adb44c_r.jpg)

## FRN(2019)

论文：Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks

FRN仍然是基于小batchsize会退化性能的问题改进的。

FRN由两个组件构成，一个是Filter Response Normalization (FRN)，一个是Thresholded Linear Unit (TLU)。

前者跟Instance Normalization非常相似，也是基于单样本单通道，所不同的是IN减去了均值，再除以标准差。而FRN没有减去均值。作者给出的理由如下：虽然减去均值是归一化方案的正常操作，但对于batch independent的归一化方案来说，它是任意的，没有任何理由。

TLU则是在ReLU的基础上加了一个阈值，这个阈值是可学习的参数。这是考虑到FRN没有减去均值的操作，这可能使得归一化的结果任意地偏移0，如果FRN之后是ReLU激活层，可能产生很多0值，这对于模型训练和性能是不利的。

![](https://pic1.zhimg.com/v2-2855b481980a0b2504862dfecccc67a4_r.jpg)

FRN实现算法

![](https://pic2.zhimg.com/v2-b7ca4995ae496b6181f07591a9fbe6fd_r.jpg)

实验效果

![](https://pic3.zhimg.com/v2-f6d066c2ee78abbf33645d971bdfd33e_r.jpg)

## Cross-Iteration BN(2020)

论文：Cross-Iteration Batch Normalization

CBN的主要思想在于将前k-1个iteration的样本参与当前均值和方差的计算。但由于前k-1次iteration的数据更新，因此无法直接拿来使用。论文提出了一个处理方式是通过泰勒多项式来近似计算出前k-1次iteration的数据。

![](https://pic3.zhimg.com/v2-09e238056ec00aa5fe39802358379aea_r.jpg)

​ 在Yolo_v4中还提出改进版CmBN，在每个batch中只统计四个mini-batches的数据，并在第四个mini-batch后才更新权重，尺度缩放和偏移。

实验效果

![](https://pic2.zhimg.com/v2-520034e70ea6d1bf26e9e6ffa849fc25_r.jpg)

## 总结

本文介绍了目前比较经典的归一化方法，其中大部分都是针对BN改进而来，本文比较详尽地介绍了它们的主要思想，改进方式，以及应用场景，部分方法并没有详细介绍实现细节，对于感兴趣或有需要的读者请自行阅读论文原文。

除了以上方法外，还有很多归一化方法，例如Eval Norm，Normalization propagation，Normalizing the normalizers等。但这些方法并不常用，这里不作赘述
