# 1. 梯度下降法(Gradient Descent)

在微积分中，对多元函数的参数求 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 偏导数，把求得的各个参数的导数以向量的形式写出来就是梯度。梯度就是函数变化最快的地方。梯度下降是迭代法的一种，在求解机器学习算法的模型参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 时，即无约束问题时，梯度下降是最常采用的方法之一。顾名思义，**梯度下降法的计算过程就是沿梯度下降的方向求解极小值，也可以沿梯度上升方向求解最大值。** 假设模型参数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，损失函数为 ![[公式]](https://www.zhihu.com/equation?tex=J%5Cleft%28%5Ctheta%5Cright%29) ，损失函数 ![[公式]](https://www.zhihu.com/equation?tex=J%5Cleft%28%5Ctheta%5Cright%29) 关于参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的偏导数，也就是梯度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown+_%7B%5Ctheta%7DJ%5Cleft+%28+%5Ctheta++%5Cright+%29) *，学习率为* ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) *，则使用梯度下降法更新参数为：*

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-%5Calpha+%5Ccdot+%5Ctriangledown+_%7B%5Ctheta%7DJ%5Cleft+%28+%5Ctheta++%5Cright+%29+)

梯度下降法目前主要分为三种方法,区别在于每次参数更新时计算的样本数据量不同：批量梯度下降法(BGD, Batch Gradient Descent)，随机梯度下降法(SGD, Stochastic Gradient Descent)及小批量梯度下降法(Mini-batch Gradient Descent)。

## 1.1 批量梯度下降法BGD

假设训练样本总数为n，样本为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B++%5Cleft%28+x%5E%7B1%7D%2Cy%5E%7B1%7D+%5Cright%29%2C+%5Ccdots%2C+%5Cleft%28x%5E%7Bn%7D%2C+y%5E%7Bn%7D%5Cright%29+%5Cright%5C%7D) ，模型参数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，损失函数为 ![[公式]](https://www.zhihu.com/equation?tex=J%5Cleft%28%5Ctheta%5Cright%29) ，在第i对样本 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%28+x%5E%7Bi%7D%2Cy%5E%7Bi%7D+%5Cright+%29) 上损失函数关于参数的梯度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_%7B%5Ctheta%7DJ_%7Bi%7D%5Cleft%28%5Ctheta%2C+x%5E%7Bi%7D%2C+y%5E%7Bi%7D+%5Cright%29) , 学习率为 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)，则使用BGD更新参数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C++%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-%5Calpha_%7Bt%7D+%5Ccdot+%5Cfrac%7B1%7D%7Bn%7D%5Ccdot+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Ctriangledown_%7B%5Ctheta%7DJ_%7Bi%7D%5Cleft%28%5Ctheta%2C+x%5E%7Bi%7D%2C+y%5E%7Bi%7D+%5Cright%29+)

由上式可以看出，每进行一次参数更新，需要计算整个数据样本集，因此导致批量梯度下降法的速度会比较慢，尤其是数据集非常大的情况下，收敛速度就会非常慢，但是由于每次的下降方向为总体平均梯度，它得到的会是一个全局最优解。

## 1.2 随机梯度下降法SGD

随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%28+x%5E%7Bi%7D%2Cy%5E%7Bi%7D%5Cright+%29) 计算其梯度，参数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-%5Calpha+%5Ccdot+%5Ctriangledown_%7B%5Ctheta%7DJ_%7Bi%7D%5Cleft%28%5Ctheta%2C+x%5E%7Bi%7D%2C+y%5E%7Bi%7D+%5Cright%29+)

可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。

## 1.3 小批量梯度下降法

小批量梯度下降法就是结合BGD和SGD的折中，对于含有n个训练样本的数据集，每次参数更新，选择一个大小为m ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%28+m+%3C+n+%5Cright+%29) 的mini-batch数据样本计算其梯度，其参数更新公式如下：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C++%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-%5Calpha+%5Ccdot+%5Cfrac%7B1%7D%7Bm%7D+%5Ccdot+%5Csum_%7Bi%3Dx%7D%5E%7Bi%3Dx%2Bm-1%7D+%5Ccdot+%5Ctriangledown_%7B%5Ctheta%7DJ_%7Bi%7D%5Cleft%28%5Ctheta%2C+x%5E%7Bi%7D%2C+y%5E%7Bi%7D+%5Cright%29+)

小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法。

```text
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
train_op = optimizer.minimize(loss, global_step=global_step)
```

**SGD缺点：**

- 选择合适的learning rate比较困难 ，学习率太低会收敛缓慢，学习率过高会使收敛时的波动过大
- 所有参数都是用同样的learning rate
- SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点

## 1.4 三种gradient descent对比

**Batch gradient descent**的收敛速度太慢，而且会大量多余的计算(比如计算相似的样本)。

**Stochastic gradient descent**虽然大大加速了收敛速度，但是它的梯度下降的波动非常大(high variance)。

**Mini-batch gradient descent**中和了2者的优缺点，所以SGD算法通常也默认是Mini-batch gradient descent。

**【Mini-batch gradient descent的缺点】**

然而Mini-batch gradient descent也不能保证很好地收敛。主要有以下缺点：

- **选择一个合适的learning rate是非常困难的**
  
  学习率太低会收敛缓慢，学习率过高会使收敛时的波动过大。

- **所有参数都是用同样的learning rate**
  
  对于稀疏数据或特征，有时我们希望对于不经常出现的特征的参数更新快一些，对于常出现的特征更新慢一些。这个时候SGD就不能满足要求了。

- **sgd容易收敛到局部最优解，并且在某些情况可能被困在鞍点**
  
  在合适的初始化和step size的情况下，鞍点的影响没那么大。

# 2. 动量优化法

动量优化方法引入物理学中的动量思想，**加速梯度下降**，有Momentum和Nesterov两种算法。当我们将一个小球从山上滚下来，没有阻力时，它的动量会越来越大，但是如果遇到了阻力，速度就会变小，动量优化法就是借鉴此思想，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡。

## 2.1 Momentum

momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。假设 ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D) 表示t时刻的动量， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 表示动量因子，通常取值0.9或者近似值，在SGD的基础上增加动量，则参数更新公式如下：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+m_%7Bt%2B1%7D%3D%5Cmu+%5Ccdot+m_%7Bt%7D+%2B+%5Calpha+%5Ccdot+%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_%7Bt%7D+-+m_%7Bt%2B1%7D)

在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。总而言之，momentum能够加速SGD收敛，抑制震荡。

**【特点】**

- 参数下降初期，加上前一次参数更新值；如果前后2次下降方向一致，乘上较大的μ能够很好的加速。
- 参数下降中后期，在局部最小值附近来回震荡时，gradient→0，μ使得更新幅度增大，跳出陷阱。
- 在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。
- **总而言之，momentum能够加速SGD收敛，抑制震荡**。

## 2.2 NAG（Nesterov accelerated gradient）

momentum保留了上一时刻的梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29) ，对其没有进行任何改变，NAG是momentum的改进，在梯度更新时做一个矫正，具体做法就是在当前的梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29) 上添加上一时刻的动量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+%5Ccdot+m_%7Bt%7D) ，梯度改变为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+-+%5Cmu+%5Ccdot+m_%7Bt%7D%5Cright%29) 。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C++m_%7Bt%2B1%7D%3D%5Cmu+%5Ccdot+m_%7Bt%7D+%2B+%5Calpha+%5Ccdot+%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+-+%5Cmu+%5Ccdot+m_%7Bt%7D+%5Cright%29+%5C%5C++%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_%7Bt%7D+-+m_%7Bt%2B1%7D+)

加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。 下图是momentum和nesterrov的对比表述图：

![](https://pic2.zhimg.com/v2-51454083960f1826e2f5a5af2b6271cd_r.jpg)

momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量)

# 3. 自适应学习率优化算法

在机器学习中，学习率是一个非常重要的超参数，但是学习率是非常难确定的，虽然可以通过多次训练来确定合适的学习率，但是一般也不太确定多少次训练能够得到最优的学习率，玄学事件，对人为的经验要求比较高，所以是否存在一些策略自适应地调节学习率的大小，从而提高训练速度。 目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。

## 3.1 AdaGrad

定义参数：全局学习率$\delta$，一般会选择 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%3D0.01) ; 一个极小的常量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) ，通常取值10e-8,目的是为了分母为0; 梯度加速变量(gradient accumulation variable) ![[公式]](https://www.zhihu.com/equation?tex=r) 。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C++g+%5Cleftarrow++%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C%5C+++r+%5Cleftarrow+r+%2B+g%5E%7B2%7D+%5C%5C++%5Ctriangle+%5Ctheta+%5Cleftarrow++%5Cfrac%7B%5Cdelta+%7D%7B%5Csqrt%7Br+%2B+%5Cepsilon%7D%7D%5Ccdot+g+%5C%5C+%5Ctheta+%5Cleftarrow+%5Ctheta+-+%5Ctriangle+%5Ctheta+)

从上式可以看出，梯度加速变量r为t时刻前梯度的平方和 ![[公式]](https://www.zhihu.com/equation?tex=r+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D+g_%7Bi%7D%5E%7B2%7D) , 那么参数更新量 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctriangle+%5Ctheta%3D%5Cdelta+%5Ccdot+%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bt%7Dg_%7Bi%7D%5E%7B2%7D+%2B+%5Cdelta%7D%7D+%5Ccdot+g) ，将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bt%7Dg_%7Bi%7D%5E%7B2%7D+%2B+%5Cdelta%7D%7D) 看成一个约束项regularizer. 在前期，梯度累计平方和比较小，也就是r相对较小，则约束项较大，这样就能够放大梯度, 参数更新量变大; 随着迭代次数增多，梯度累计平方和也越来越大，即r也相对较大，则约束项变小，这样能够缩小梯度，参数更新量变小。

**【特点】**

- 前期nt较小的时候，regularizer较大，能够放大梯度
- 后期nt较大的时候，regularizer较小，能够缩小梯度
- 中后期，分母上梯度平方的累加会越来越大，使gradient→0，使得训练提前结束。

**【缺点】**  

- 由公式可以看出，仍依赖于人工设置的一个全局学习率 η  
  - η 设置过大的话，会使regularizer过于敏感，对梯度调节太大。  
  - **最重要的是，中后期分母上的梯度平方累加会越来越大，使gradient→0，使得训练提前结束，无法继续学习。**

**缺点：**

- 仍需要手工设置一个全局学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) , 如果 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) 设置过大的话，会使regularizer过于敏感，对梯度的调节太大
- 中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习

## 3.3 Adadelta

Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值

![[公式]](https://www.zhihu.com/equation?tex=g+%5Cleftarrow+%5Ctriangledown_%7B%5Ctheta%7D+%5C%5C+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C++n_%7Bt%7D+%5Cleftarrow+v+%5Ccdot+n_%7Bt-1%7D+%2B+%5Cleft%281+-v+%5Cright%29+%5Ccdot+g_%7Bt%7D%5E%7B2%7D+%5C%5C+++%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-+%5Cfrac%7B%5Cdelta%7D%7B%5Csqrt%7Bn_%7Bt%7D+%2B+%5Cepsilon%7D%7D+%5Ccdot+g_%7Bt%7D+)

从上式中可以看出，Adadelta其实还是依赖于全局学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) ，但是作者做了一定处理，经过近似牛顿迭代法之后

![[公式]](https://www.zhihu.com/equation?tex=E+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt%7D+%5Cleftarrow+%5Crho+%5Ccdot+E+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt-1%7D%2B+%5Cleft%281+-+%5Crho+%5Cright%29+%5Ccdot+g_%7Bt%7D%5E%7B2%7D+%5C%5C+%5Ctriangle+%5Ctheta+%5Cleftarrow+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7D+%5Ctriangle+%5Ctheta_%7Br%7D%7D%7B%5Csqrt%7BE+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt%7D+%2B+%5Cepsilon%7D%7D+)

此时可以看出Adadelta已经不依赖全局learning rate了。

**特点：**

- 训练初中期，加速效果不错，很快。
- 训练后期，反复在局部最小值附近抖动。

## 3.4 RMSprop

RMSProp算法修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好。设定参数：全局初始率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) , 默认设为0.001; decay rate ![[公式]](https://www.zhihu.com/equation?tex=%5Crho) ,默认设置为0.9,一个极小的常量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) ，通常为10e-6

![[公式]](https://www.zhihu.com/equation?tex=+g+%5Cleftarrow++%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C%5C+++E+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt%7D+%5Cleftarrow+%5Crho+%5Ccdot+E+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt-1%7D%2B+%5Cleft%281+-+%5Crho+%5Cright%29+%5Ccdot+g_%7Bt%7D%5E%7B2%7D+%5C%5C+%5Ctriangle+%5Ctheta+%5Cleftarrow++%5Cfrac%7B%5Cdelta+%7D%7B%5Csqrt%7BE+%5Cleft%5B+g%5E%7B2%7D%5Cright%5D_%7Bt%7D+%2B+%5Cepsilon%7D%7D%5Ccdot+g+%5C%5C++%5Ctheta+%5Cleftarrow+%5Ctheta+-+%5Ctriangle+%5Ctheta+)

**特点：**

- 其实RMSprop依然依赖于全局学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta)
- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标(包括季节性和周期性)——对于RNN效果很好

## 3.5 Adam: Adaptive Moment Estimation

Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。 默认参数值设定为： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_%7B1%7D+%3D+0.9%2C+%5Cbeta_%7B2%7D+%3D+0.999%2C+%5Cepsilon%3D10%5E%7B-8%7D)

![[公式]](https://www.zhihu.com/equation?tex=g+%5Cleftarrow+%5Ctriangledown_%7B%5Ctheta%7D+%5C%5C++J+%5Cleft%28%5Ctheta+%5Cright%29+%5C++m_%7Bt%7D+%5Cleftarrow+%5Cbeta_%7B1%7D+%5Ccdot+m_%7Bt-1%7D+%2B+%5Cleft%281+-+%5Cbeta_%7B1%7D+%5Cright%29+%5Ccdot+g_%7Bt%7D+%5C%5C++v_%7Bt%7D+%5Cleftarrow+%5Cbeta_%7B2%7D+%5Ccdot+v_%7Bt-1%7D+%2B+%5Cleft%28+1+-+%5Cbeta_%7B2%7D+%5Cright%29+%5Ccdot+g_%7Bt%7D%5E%7B2%7D++%5C%5C+%5Chat%7Bm%7D%7Bt%7D+%5Cleftarrow+%5Cfrac%7Bm%7Bt%7D%7D%7B1+-+%5Cbeta_%7B1%7D%5E%7Bt%7D%7D+%5C%5C++%5Chat%7Bv%7D%7Bt%7D+%5Cleftarrow+%5Cfrac%7Bv%7Bt%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D+%5C%5C+%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-+%5Cfrac%7B%5Cdelta%7D%7B%5Cepsilon+%2B+%5Csqrt%7B%5Chat%7Bv_%7Bt%7D%7D%7D%7D+%5Ccdot+%5Chat%7Bm%7D_%7Bt%7D)

*其中，* ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=n_%7Bt%7D) 分别是对梯度的一阶矩估计和二阶矩估计； ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm%7D_%7Bt%7D) *，* ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bn%7D_%7Bt%7D) 是对 ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D)， ![[公式]](https://www.zhihu.com/equation?tex=n_%7Bt%7D)的偏差校正，这样可以近似为对期望的无偏估计

**特点：**

- Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。
- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化问题——适用于大数据集和高维空间。

## 3.6 Adamax

Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。公式上的变化如下：

![[公式]](https://www.zhihu.com/equation?tex=n_t%3Dmax%28%5Cnu%2An_%7Bt-1%7D%2C%7Cg_t%7C%29)  

![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7Bx%7D%3D-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7Bn_t%2B%5Cepsilon%7D%2A%5Ceta)

可以看出，Adamax学习率的边界范围更简单

## 3.7 Nadam

Nadam类似于带有Nesterov动量项的Adam。公式如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bg_t%7D%3D%5Cfrac%7Bg_t%7D%7B1-%5CPi_%7Bi%3D1%7D%5Et%5Cmu_i%7D)  

![[公式]](https://www.zhihu.com/equation?tex=m_t%3D%5Cmu_t%2Am_%7Bt-1%7D%2B%281-%5Cmu_t%29%2Ag_t)  

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm_t%7D%3D%5Cfrac%7Bm_t%7D%7B1-%5CPi_%7Bi%3D1%7D%5E%7Bt%2B1%7D%5Cmu_i%7D)  

![[公式]](https://www.zhihu.com/equation?tex=n_t%3D%5Cnu%2An_%7Bt-1%7D%2B%281-%5Cnu%29%2Ag_t%5E2)  

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bn_t%7D%3D%5Cfrac%7Bn_t%7D%7B1-%5Cnu%5Et%7D)![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7Bm_t%7D%3D%281-%5Cmu_t%29%2A%5Chat%7Bg_t%7D%2B%5Cmu_%7Bt%2B1%7D%2A%5Chat%7Bm_t%7D)  

![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Ceta%2A%5Cfrac%7B%5Cbar%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D)

可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。

# 4. 优化算法的选择

- **对于稀疏数据，尽量使用学习率可自适应的算法，不用手动调节，而且最好采用默认参数**
- **SGD通常训练时间最长，但是在好的初始化和学习率调度方案下，结果往往更可靠。但SGD容易困在鞍点，这个缺点也不能忽略。**
- **如果在意收敛的速度，并且需要训练比较深比较复杂的网络时，推荐使用学习率自适应的优化方法。**
- **Adagrad，Adadelta和RMSprop是比较相近的算法，表现都差不多。**
- **在能使用带动量的RMSprop或者Adam的地方，使用Nadam往往能取得更好的效果。**
